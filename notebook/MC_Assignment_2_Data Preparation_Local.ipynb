{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulator Imports\n",
    "import pandas as pd\n",
    "\n",
    "# Normal Imports\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crunching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'buy', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'communicate', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'fun', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'mother', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really', 'really']\n"
     ]
    }
   ],
   "source": [
    "#Getting all the files uploaded by the professor\n",
    "folders = ['buy','communicate','fun','hope','mother','really']\n",
    "dfs = []\n",
    "signs = []\n",
    "for folder in folders:\n",
    "    for file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, file)\n",
    "        \n",
    "        # Converting into Dataframes\n",
    "        df = pd.read_csv(file_path)[[\"leftShoulder_x\", \"leftShoulder_y\", \"rightShoulder_x\", \"rightShoulder_y\", \"leftElbow_x\", \"leftElbow_y\", \"rightElbow_x\", \"rightElbow_y\", \"leftWrist_x\", \"leftWrist_y\", \"rightWrist_x\", \"rightWrist_y\"]]\n",
    "        \n",
    "        # Creating master list of Dataframes\n",
    "        dfs.append(df)\n",
    "        \n",
    "        # Sign from filename\n",
    "        signs.append(folder)\n",
    "print(signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum-33 & maximum-232 number of rows in a CSV input file.\n"
     ]
    }
   ],
   "source": [
    "print(\"minimum-{} & maximum-{} number of rows in a CSV input file.\".format(min([len(i) for i in dfs]),max([len(i) for i in dfs])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1 [Not Followed]\n",
    "\n",
    "---\n",
    "Take the minimum length of a dataframe (limit set to 140) and cut down all other dataframes from the top and bottom to be the minimum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making All the Dataframes of the Minimum Length - Inplace\n",
    "minlen = min([len(df) for df in dfs])\n",
    "c = 0\n",
    "for i in range(len(dfs)):\n",
    "  c += 1\n",
    "  dflen = len(dfs[i])\n",
    "  if(dflen>minlen):\n",
    "    dellen = dflen-minlen\n",
    "    startlen = int(dellen/2)\n",
    "    endlen = dflen - (dellen - startlen)\n",
    "    dfs[i] = dfs[i].iloc[startlen:endlen].reset_index(drop=True)\n",
    "  print(c, end='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2 [ Followed ]\n",
    "\n",
    "\n",
    "---\n",
    "Getting the maximum length of rows in any given CSV input file, and imputing all other dataframes with mean value rows till they are the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 2\n",
    "maxlen = max([len(df) for df in dfs])\n",
    "c = 0\n",
    "for i in range(len(dfs)):\n",
    "  c += 1\n",
    "  dflen = len(dfs[i])\n",
    "  #print(i)\n",
    "  if(len(dfs[i])<maxlen):\n",
    "    dfs[i] = dfs[i].append([pd.Series()]*(maxlen-dflen), ignore_index=True)\n",
    "  dfs[i].fillna(dfs[i].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing String Values of Labels to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "mapdict = {'communicate':0, 'communication':0, 'hope':1, 'mother':2, 'really':3, 'fun':4, 'buy':5}\n",
    "for i in range(len(signs)):\n",
    "  if(signs[i]=='communication'):\n",
    "    signs[i] = (mapdict[signs[i]])\n",
    "  else:\n",
    "    signs[i] = (mapdict[signs[i]])\n",
    "print(signs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the master list of all above entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.stack(dfs)\n",
    "y_all = np.array(signs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Yfile_final.pkl', 'wb')\n",
    "pickle.dump(y_all, f)\n",
    "f.close()\n",
    "f = open('Xfile_final.pkl', 'wb')\n",
    "pickle.dump(X_all, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
