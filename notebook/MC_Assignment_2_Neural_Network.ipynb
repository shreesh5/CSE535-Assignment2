{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcrCgBnukjvm"
   },
   "source": [
    "# Import Statements\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "kTW8rxoOGMO-",
    "outputId": "6d2eca4f-67ec-484d-e9ed-c84706143121"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Visualization Statements\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normal Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Neural Network Imports\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LeakyReLU, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Data Manipulator/Scaler Imports\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Upload from Local Drive\n",
    "from google.colab import files\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SbrK2lukkrXN"
   },
   "source": [
    "# Import Data from Local Drive\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "c5m1uVpvdFje",
    "outputId": "a5474744-a81a-4ab1-afdd-eb080c5472c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-1de941c7-d0f8-4cde-bfb5-1491fc08a2b5\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-1de941c7-d0f8-4cde-bfb5-1491fc08a2b5\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Xfileupdated to Xfileupdated\n",
      "Saving Yfileupdated to Yfileupdated\n"
     ]
    }
   ],
   "source": [
    "# Run this cell, and choose the two files Xfileupdated, Yfileupdated, for smooth functioning of the notebook\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1j2EfKAIdSA4"
   },
   "outputs": [],
   "source": [
    "# Contains a 3D Matrix of nsamples*nx*ny (where ny=12 features, nx=232 rows)\n",
    "X_all = pickle.load(open('Xfileupdated', 'rb'))\n",
    "\n",
    "# Contains a 2D Matrix of nsamples*2 (where each list contains ['fun',1], ['mother',2] kind of data)\n",
    "Y_all = pickle.load(open('Yfileupdated', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDpxeY3wkWq6"
   },
   "source": [
    "# Neural Network Parameters\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Rb301wrB5Hs"
   },
   "outputs": [],
   "source": [
    "# Remove those warnings!\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6AMdn12Il-R5"
   },
   "source": [
    "## Modifying & Scaling the Matrices\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0SjuU6Uw0FtS"
   },
   "outputs": [],
   "source": [
    "# Reshaping a nsamples*nx*ny matrix(3D) to nsamples*(nx*ny) matrix(2D)\n",
    "nsamples, nx, ny = X_all.shape\n",
    "flatten_X_all = X_all.reshape((nsamples,nx*ny))\n",
    "\n",
    "# Changing ['fun',2] -> 2, ['mother',3] -> 3, etc.\n",
    "flatten_Y_all = [i[1] for i in Y_all]\n",
    "\n",
    "# Neural Network needs one-hot vector to work properly.\n",
    "# This converts 2 -> [0, 0, 1, 0, 0, 0], 3 -> [0, 0, 0, 1, 0, 0], etc.\n",
    "flatten_Y_all = to_categorical(flatten_Y_all)\n",
    "\n",
    "# Neural Network also needs all values between [0,1) to even start doing optimized calculations\n",
    "scaler = MinMaxScaler()\n",
    "flatten_X_all = scaler.fit_transform(flatten_X_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sUybrJvk8lHR"
   },
   "source": [
    "## Creating Train, Validation, and Test Data\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PP6aHGzYlg79"
   },
   "outputs": [],
   "source": [
    "# Splitting \n",
    "X_A, X_test, Y_A, y_test = train_test_split(flatten_X_all, flatten_Y_all, test_size=0.01)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_A, Y_A, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KPUDo54J9nhb",
    "outputId": "081a4da3-d509-4c3b-b312-a7f53d3f3611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261, 2784) (29, 2784) (125, 2784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_validation.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aoTZYJtNl4tJ"
   },
   "source": [
    "## Preparing the model\n",
    "\n",
    "---\n",
    "\n",
    "Tried a lot of things. The following implementation worked the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "npYqgwaD9Gp2",
    "outputId": "0cb5407e-b172-4c96-b05d-c40d9213fe8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the constructor\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer - 232*12 = 2784\n",
    "model.add(Dense(256, activation='relu', input_shape=(2784,)))\n",
    "\n",
    "# Changing ReLU to LeakyReLU (Does not let weights ever turn to 0)\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "# Hidden Layer 1 - Added L2 regularization (because the graphs below were showing huge fluctuations)\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.03), bias_regularizer=l2(0.02)))\n",
    "\n",
    "# Add an output layer  (Returns a [0, 0, 1, 0, 0, 0] kind of softmax output)\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "J_8nvP9SDXUz",
    "outputId": "2fee1169-2a27-4f8d-feaa-35d3208fbbb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               712960    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 746,630\n",
      "Trainable params: 746,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_L_DvF_m_oY"
   },
   "source": [
    "## Compile Model & Fit Train Data\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kGLLz1iz9feC",
    "outputId": "a5054f93-3f68-4857-817e-92d102988217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 261 samples, validate on 29 samples\n",
      "Epoch 1/600\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "261/261 [==============================] - 1s 4ms/step - loss: 6.7656 - acc: 0.1456 - val_loss: 5.8065 - val_acc: 0.5172\n",
      "Epoch 2/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 5.3325 - acc: 0.2414 - val_loss: 4.8766 - val_acc: 0.2069\n",
      "Epoch 3/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 4.4153 - acc: 0.2605 - val_loss: 4.2109 - val_acc: 0.2069\n",
      "Epoch 4/600\n",
      "261/261 [==============================] - 0s 633us/step - loss: 3.7690 - acc: 0.2797 - val_loss: 4.0008 - val_acc: 0.1724\n",
      "Epoch 5/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 3.3369 - acc: 0.3372 - val_loss: 2.9753 - val_acc: 0.3448\n",
      "Epoch 6/600\n",
      "261/261 [==============================] - 0s 653us/step - loss: 2.7816 - acc: 0.4521 - val_loss: 2.6657 - val_acc: 0.4828\n",
      "Epoch 7/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 2.4295 - acc: 0.4291 - val_loss: 2.5097 - val_acc: 0.3103\n",
      "Epoch 8/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 2.3430 - acc: 0.4215 - val_loss: 2.7213 - val_acc: 0.1379\n",
      "Epoch 9/600\n",
      "261/261 [==============================] - 0s 652us/step - loss: 2.1011 - acc: 0.4521 - val_loss: 1.9890 - val_acc: 0.6552\n",
      "Epoch 10/600\n",
      "261/261 [==============================] - 0s 654us/step - loss: 1.8264 - acc: 0.5594 - val_loss: 1.9943 - val_acc: 0.4138\n",
      "Epoch 11/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 1.7982 - acc: 0.4943 - val_loss: 1.8794 - val_acc: 0.5517\n",
      "Epoch 12/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 1.6760 - acc: 0.5057 - val_loss: 1.6553 - val_acc: 0.5862\n",
      "Epoch 13/600\n",
      "261/261 [==============================] - 0s 664us/step - loss: 1.5544 - acc: 0.5632 - val_loss: 1.7151 - val_acc: 0.5172\n",
      "Epoch 14/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 1.5831 - acc: 0.5134 - val_loss: 1.5709 - val_acc: 0.4483\n",
      "Epoch 15/600\n",
      "261/261 [==============================] - 0s 672us/step - loss: 1.5007 - acc: 0.5747 - val_loss: 1.5718 - val_acc: 0.6207\n",
      "Epoch 16/600\n",
      "261/261 [==============================] - 0s 645us/step - loss: 1.6495 - acc: 0.4904 - val_loss: 1.9041 - val_acc: 0.3793\n",
      "Epoch 17/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 1.4895 - acc: 0.5364 - val_loss: 1.5773 - val_acc: 0.6897\n",
      "Epoch 18/600\n",
      "261/261 [==============================] - 0s 706us/step - loss: 1.4702 - acc: 0.5057 - val_loss: 1.5406 - val_acc: 0.4828\n",
      "Epoch 19/600\n",
      "261/261 [==============================] - 0s 637us/step - loss: 1.3320 - acc: 0.6245 - val_loss: 1.4001 - val_acc: 0.6897\n",
      "Epoch 20/600\n",
      "261/261 [==============================] - 0s 623us/step - loss: 1.3294 - acc: 0.6130 - val_loss: 1.3785 - val_acc: 0.7586\n",
      "Epoch 21/600\n",
      "261/261 [==============================] - 0s 657us/step - loss: 1.3186 - acc: 0.6169 - val_loss: 1.4014 - val_acc: 0.6552\n",
      "Epoch 22/600\n",
      "261/261 [==============================] - 0s 617us/step - loss: 1.4282 - acc: 0.5632 - val_loss: 1.3724 - val_acc: 0.5517\n",
      "Epoch 23/600\n",
      "261/261 [==============================] - 0s 647us/step - loss: 1.2702 - acc: 0.5862 - val_loss: 1.2377 - val_acc: 0.7586\n",
      "Epoch 24/600\n",
      "261/261 [==============================] - 0s 669us/step - loss: 1.1632 - acc: 0.6858 - val_loss: 1.3032 - val_acc: 0.6897\n",
      "Epoch 25/600\n",
      "261/261 [==============================] - 0s 654us/step - loss: 1.2206 - acc: 0.6513 - val_loss: 1.3523 - val_acc: 0.5862\n",
      "Epoch 26/600\n",
      "261/261 [==============================] - 0s 670us/step - loss: 1.6360 - acc: 0.4751 - val_loss: 1.5786 - val_acc: 0.5862\n",
      "Epoch 27/600\n",
      "261/261 [==============================] - 0s 664us/step - loss: 1.2709 - acc: 0.6054 - val_loss: 1.4053 - val_acc: 0.6207\n",
      "Epoch 28/600\n",
      "261/261 [==============================] - 0s 605us/step - loss: 1.2025 - acc: 0.6398 - val_loss: 1.2457 - val_acc: 0.7931\n",
      "Epoch 29/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 1.1704 - acc: 0.6322 - val_loss: 1.2483 - val_acc: 0.6207\n",
      "Epoch 30/600\n",
      "261/261 [==============================] - 0s 656us/step - loss: 1.1543 - acc: 0.6130 - val_loss: 1.2514 - val_acc: 0.6552\n",
      "Epoch 31/600\n",
      "261/261 [==============================] - 0s 655us/step - loss: 1.2558 - acc: 0.5862 - val_loss: 1.3485 - val_acc: 0.6207\n",
      "Epoch 32/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 1.2124 - acc: 0.6169 - val_loss: 1.3140 - val_acc: 0.5517\n",
      "Epoch 33/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 1.1587 - acc: 0.6207 - val_loss: 1.2079 - val_acc: 0.7241\n",
      "Epoch 34/600\n",
      "261/261 [==============================] - 0s 707us/step - loss: 1.0335 - acc: 0.7395 - val_loss: 1.1817 - val_acc: 0.6552\n",
      "Epoch 35/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 0.9942 - acc: 0.7241 - val_loss: 1.1485 - val_acc: 0.6897\n",
      "Epoch 36/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 1.0114 - acc: 0.7011 - val_loss: 1.1027 - val_acc: 0.6897\n",
      "Epoch 37/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 1.1200 - acc: 0.6207 - val_loss: 1.2170 - val_acc: 0.5862\n",
      "Epoch 38/600\n",
      "261/261 [==============================] - 0s 656us/step - loss: 1.0431 - acc: 0.6973 - val_loss: 1.0945 - val_acc: 0.7586\n",
      "Epoch 39/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.9693 - acc: 0.7011 - val_loss: 1.0357 - val_acc: 0.7586\n",
      "Epoch 40/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 1.0166 - acc: 0.6858 - val_loss: 1.0958 - val_acc: 0.6897\n",
      "Epoch 41/600\n",
      "261/261 [==============================] - 0s 636us/step - loss: 0.9696 - acc: 0.7356 - val_loss: 1.0689 - val_acc: 0.7931\n",
      "Epoch 42/600\n",
      "261/261 [==============================] - 0s 664us/step - loss: 0.9442 - acc: 0.7280 - val_loss: 1.1113 - val_acc: 0.7241\n",
      "Epoch 43/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 0.9810 - acc: 0.6590 - val_loss: 1.5064 - val_acc: 0.5517\n",
      "Epoch 44/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 1.5716 - acc: 0.4789 - val_loss: 1.2572 - val_acc: 0.6207\n",
      "Epoch 45/600\n",
      "261/261 [==============================] - 0s 647us/step - loss: 1.2356 - acc: 0.5824 - val_loss: 1.2464 - val_acc: 0.6207\n",
      "Epoch 46/600\n",
      "261/261 [==============================] - 0s 721us/step - loss: 1.0174 - acc: 0.7241 - val_loss: 1.0897 - val_acc: 0.7931\n",
      "Epoch 47/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.9402 - acc: 0.7165 - val_loss: 1.0142 - val_acc: 0.7586\n",
      "Epoch 48/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 0.9479 - acc: 0.6897 - val_loss: 1.0378 - val_acc: 0.7586\n",
      "Epoch 49/600\n",
      "261/261 [==============================] - 0s 660us/step - loss: 0.9066 - acc: 0.7280 - val_loss: 1.0125 - val_acc: 0.7931\n",
      "Epoch 50/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.8934 - acc: 0.7356 - val_loss: 1.2096 - val_acc: 0.7241\n",
      "Epoch 51/600\n",
      "261/261 [==============================] - 0s 688us/step - loss: 1.0529 - acc: 0.6245 - val_loss: 1.6616 - val_acc: 0.4483\n",
      "Epoch 52/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 1.1917 - acc: 0.6015 - val_loss: 1.4640 - val_acc: 0.5172\n",
      "Epoch 53/600\n",
      "261/261 [==============================] - 0s 694us/step - loss: 1.0414 - acc: 0.6360 - val_loss: 1.2030 - val_acc: 0.6552\n",
      "Epoch 54/600\n",
      "261/261 [==============================] - 0s 662us/step - loss: 0.9487 - acc: 0.7088 - val_loss: 1.0279 - val_acc: 0.7931\n",
      "Epoch 55/600\n",
      "261/261 [==============================] - 0s 664us/step - loss: 0.9890 - acc: 0.6552 - val_loss: 0.9484 - val_acc: 0.7241\n",
      "Epoch 56/600\n",
      "261/261 [==============================] - 0s 673us/step - loss: 0.8493 - acc: 0.6973 - val_loss: 0.9424 - val_acc: 0.7241\n",
      "Epoch 57/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 0.8087 - acc: 0.7395 - val_loss: 1.6157 - val_acc: 0.6552\n",
      "Epoch 58/600\n",
      "261/261 [==============================] - 0s 641us/step - loss: 1.6830 - acc: 0.4866 - val_loss: 1.3934 - val_acc: 0.5517\n",
      "Epoch 59/600\n",
      "261/261 [==============================] - 0s 633us/step - loss: 1.1024 - acc: 0.6437 - val_loss: 1.0764 - val_acc: 0.6897\n",
      "Epoch 60/600\n",
      "261/261 [==============================] - 0s 603us/step - loss: 0.9530 - acc: 0.6820 - val_loss: 1.0298 - val_acc: 0.6897\n",
      "Epoch 61/600\n",
      "261/261 [==============================] - 0s 659us/step - loss: 0.8943 - acc: 0.7088 - val_loss: 0.9670 - val_acc: 0.7586\n",
      "Epoch 62/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.8598 - acc: 0.7356 - val_loss: 1.0765 - val_acc: 0.5862\n",
      "Epoch 63/600\n",
      "261/261 [==============================] - 0s 615us/step - loss: 0.9915 - acc: 0.6360 - val_loss: 1.0589 - val_acc: 0.7586\n",
      "Epoch 64/600\n",
      "261/261 [==============================] - 0s 653us/step - loss: 0.8573 - acc: 0.6935 - val_loss: 0.9517 - val_acc: 0.7586\n",
      "Epoch 65/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.8745 - acc: 0.6782 - val_loss: 1.0165 - val_acc: 0.6897\n",
      "Epoch 66/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.7980 - acc: 0.7663 - val_loss: 0.9744 - val_acc: 0.7586\n",
      "Epoch 67/600\n",
      "261/261 [==============================] - 0s 646us/step - loss: 0.8584 - acc: 0.7241 - val_loss: 0.9913 - val_acc: 0.6897\n",
      "Epoch 68/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.7958 - acc: 0.7126 - val_loss: 0.9724 - val_acc: 0.7241\n",
      "Epoch 69/600\n",
      "261/261 [==============================] - 0s 628us/step - loss: 0.8926 - acc: 0.6667 - val_loss: 1.0628 - val_acc: 0.6897\n",
      "Epoch 70/600\n",
      "261/261 [==============================] - 0s 666us/step - loss: 0.8130 - acc: 0.7625 - val_loss: 0.9776 - val_acc: 0.7241\n",
      "Epoch 71/600\n",
      "261/261 [==============================] - 0s 620us/step - loss: 0.8149 - acc: 0.7395 - val_loss: 0.9583 - val_acc: 0.7931\n",
      "Epoch 72/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.9307 - acc: 0.6590 - val_loss: 1.0881 - val_acc: 0.7241\n",
      "Epoch 73/600\n",
      "261/261 [==============================] - 0s 621us/step - loss: 0.8027 - acc: 0.7203 - val_loss: 1.0148 - val_acc: 0.7241\n",
      "Epoch 74/600\n",
      "261/261 [==============================] - 0s 624us/step - loss: 0.7862 - acc: 0.7548 - val_loss: 1.0621 - val_acc: 0.6897\n",
      "Epoch 75/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 0.8027 - acc: 0.6820 - val_loss: 0.9408 - val_acc: 0.7586\n",
      "Epoch 76/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.7570 - acc: 0.7280 - val_loss: 0.9687 - val_acc: 0.7586\n",
      "Epoch 77/600\n",
      "261/261 [==============================] - 0s 621us/step - loss: 0.7350 - acc: 0.7701 - val_loss: 0.8901 - val_acc: 0.7931\n",
      "Epoch 78/600\n",
      "261/261 [==============================] - 0s 646us/step - loss: 0.7494 - acc: 0.7816 - val_loss: 0.9254 - val_acc: 0.7586\n",
      "Epoch 79/600\n",
      "261/261 [==============================] - 0s 625us/step - loss: 0.9684 - acc: 0.6475 - val_loss: 1.0252 - val_acc: 0.6897\n",
      "Epoch 80/600\n",
      "261/261 [==============================] - 0s 652us/step - loss: 0.7991 - acc: 0.7088 - val_loss: 0.8564 - val_acc: 0.7931\n",
      "Epoch 81/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.7457 - acc: 0.7586 - val_loss: 0.9478 - val_acc: 0.7241\n",
      "Epoch 82/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.7061 - acc: 0.7586 - val_loss: 1.0488 - val_acc: 0.6897\n",
      "Epoch 83/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.7048 - acc: 0.7854 - val_loss: 0.8586 - val_acc: 0.8276\n",
      "Epoch 84/600\n",
      "261/261 [==============================] - 0s 647us/step - loss: 0.6931 - acc: 0.7778 - val_loss: 0.8554 - val_acc: 0.8276\n",
      "Epoch 85/600\n",
      "261/261 [==============================] - 0s 624us/step - loss: 0.6786 - acc: 0.7778 - val_loss: 0.8519 - val_acc: 0.8276\n",
      "Epoch 86/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 0.6613 - acc: 0.7778 - val_loss: 0.8678 - val_acc: 0.7931\n",
      "Epoch 87/600\n",
      "261/261 [==============================] - 0s 654us/step - loss: 0.6715 - acc: 0.7701 - val_loss: 0.8552 - val_acc: 0.7931\n",
      "Epoch 88/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.7096 - acc: 0.7471 - val_loss: 0.8272 - val_acc: 0.7931\n",
      "Epoch 89/600\n",
      "261/261 [==============================] - 0s 657us/step - loss: 0.7092 - acc: 0.7586 - val_loss: 0.8557 - val_acc: 0.7586\n",
      "Epoch 90/600\n",
      "261/261 [==============================] - 0s 664us/step - loss: 0.7509 - acc: 0.7395 - val_loss: 0.7962 - val_acc: 0.7931\n",
      "Epoch 91/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 0.8443 - acc: 0.6973 - val_loss: 1.1315 - val_acc: 0.6897\n",
      "Epoch 92/600\n",
      "261/261 [==============================] - 0s 646us/step - loss: 0.7502 - acc: 0.7203 - val_loss: 0.9058 - val_acc: 0.7241\n",
      "Epoch 93/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.6740 - acc: 0.7625 - val_loss: 0.8211 - val_acc: 0.7586\n",
      "Epoch 94/600\n",
      "261/261 [==============================] - 0s 623us/step - loss: 0.6250 - acc: 0.7816 - val_loss: 0.8594 - val_acc: 0.7931\n",
      "Epoch 95/600\n",
      "261/261 [==============================] - 0s 652us/step - loss: 0.8135 - acc: 0.7165 - val_loss: 0.8945 - val_acc: 0.7241\n",
      "Epoch 96/600\n",
      "261/261 [==============================] - 0s 661us/step - loss: 0.6973 - acc: 0.7663 - val_loss: 0.8574 - val_acc: 0.7586\n",
      "Epoch 97/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.6541 - acc: 0.7854 - val_loss: 0.8802 - val_acc: 0.7241\n",
      "Epoch 98/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.6292 - acc: 0.7931 - val_loss: 0.8216 - val_acc: 0.7586\n",
      "Epoch 99/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.8055 - acc: 0.7050 - val_loss: 0.8285 - val_acc: 0.7931\n",
      "Epoch 100/600\n",
      "261/261 [==============================] - 0s 672us/step - loss: 0.7227 - acc: 0.7165 - val_loss: 0.9784 - val_acc: 0.6897\n",
      "Epoch 101/600\n",
      "261/261 [==============================] - 0s 652us/step - loss: 0.6308 - acc: 0.8123 - val_loss: 0.7587 - val_acc: 0.7931\n",
      "Epoch 102/600\n",
      "261/261 [==============================] - 0s 679us/step - loss: 0.5868 - acc: 0.8161 - val_loss: 0.8234 - val_acc: 0.7586\n",
      "Epoch 103/600\n",
      "261/261 [==============================] - 0s 686us/step - loss: 0.5868 - acc: 0.8084 - val_loss: 0.7571 - val_acc: 0.7931\n",
      "Epoch 104/600\n",
      "261/261 [==============================] - 0s 677us/step - loss: 0.5964 - acc: 0.7931 - val_loss: 0.7945 - val_acc: 0.8276\n",
      "Epoch 105/600\n",
      "261/261 [==============================] - 0s 669us/step - loss: 0.6134 - acc: 0.7969 - val_loss: 0.8159 - val_acc: 0.7931\n",
      "Epoch 106/600\n",
      "261/261 [==============================] - 0s 756us/step - loss: 0.6277 - acc: 0.7778 - val_loss: 0.9978 - val_acc: 0.6207\n",
      "Epoch 107/600\n",
      "261/261 [==============================] - 0s 687us/step - loss: 0.6655 - acc: 0.7816 - val_loss: 0.8071 - val_acc: 0.7931\n",
      "Epoch 108/600\n",
      "261/261 [==============================] - 0s 680us/step - loss: 0.5534 - acc: 0.8314 - val_loss: 0.7768 - val_acc: 0.7931\n",
      "Epoch 109/600\n",
      "261/261 [==============================] - 0s 677us/step - loss: 0.6049 - acc: 0.7931 - val_loss: 0.7688 - val_acc: 0.7586\n",
      "Epoch 110/600\n",
      "261/261 [==============================] - 0s 669us/step - loss: 0.5658 - acc: 0.8314 - val_loss: 0.9278 - val_acc: 0.7586\n",
      "Epoch 111/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 0.5538 - acc: 0.8352 - val_loss: 0.7150 - val_acc: 0.7931\n",
      "Epoch 112/600\n",
      "261/261 [==============================] - 0s 629us/step - loss: 0.5479 - acc: 0.8391 - val_loss: 0.6987 - val_acc: 0.7931\n",
      "Epoch 113/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.5530 - acc: 0.7854 - val_loss: 0.6872 - val_acc: 0.7931\n",
      "Epoch 114/600\n",
      "261/261 [==============================] - 0s 663us/step - loss: 0.5506 - acc: 0.8199 - val_loss: 0.7334 - val_acc: 0.7931\n",
      "Epoch 115/600\n",
      "261/261 [==============================] - 0s 636us/step - loss: 0.5347 - acc: 0.8046 - val_loss: 0.6902 - val_acc: 0.8276\n",
      "Epoch 116/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.5776 - acc: 0.7969 - val_loss: 0.6627 - val_acc: 0.7931\n",
      "Epoch 117/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 0.5374 - acc: 0.8238 - val_loss: 0.7201 - val_acc: 0.8276\n",
      "Epoch 118/600\n",
      "261/261 [==============================] - 0s 621us/step - loss: 0.5625 - acc: 0.8008 - val_loss: 0.9850 - val_acc: 0.6897\n",
      "Epoch 119/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.6104 - acc: 0.7739 - val_loss: 0.9874 - val_acc: 0.7241\n",
      "Epoch 120/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.6216 - acc: 0.8046 - val_loss: 0.6800 - val_acc: 0.8276\n",
      "Epoch 121/600\n",
      "261/261 [==============================] - 0s 647us/step - loss: 0.5323 - acc: 0.8467 - val_loss: 0.6550 - val_acc: 0.7931\n",
      "Epoch 122/600\n",
      "261/261 [==============================] - 0s 620us/step - loss: 0.5235 - acc: 0.8238 - val_loss: 0.6509 - val_acc: 0.8276\n",
      "Epoch 123/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.5105 - acc: 0.8391 - val_loss: 0.8544 - val_acc: 0.7931\n",
      "Epoch 124/600\n",
      "261/261 [==============================] - 0s 652us/step - loss: 0.8497 - acc: 0.6858 - val_loss: 0.6335 - val_acc: 0.7931\n",
      "Epoch 125/600\n",
      "261/261 [==============================] - 0s 610us/step - loss: 0.6746 - acc: 0.7241 - val_loss: 0.7094 - val_acc: 0.8276\n",
      "Epoch 126/600\n",
      "261/261 [==============================] - 0s 633us/step - loss: 0.5906 - acc: 0.7778 - val_loss: 0.6794 - val_acc: 0.7931\n",
      "Epoch 127/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.5436 - acc: 0.8352 - val_loss: 0.6466 - val_acc: 0.8276\n",
      "Epoch 128/600\n",
      "261/261 [==============================] - 0s 627us/step - loss: 0.5392 - acc: 0.7969 - val_loss: 0.7007 - val_acc: 0.7586\n",
      "Epoch 129/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.5239 - acc: 0.8314 - val_loss: 0.6582 - val_acc: 0.7931\n",
      "Epoch 130/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.4988 - acc: 0.8429 - val_loss: 0.7289 - val_acc: 0.7931\n",
      "Epoch 131/600\n",
      "261/261 [==============================] - 0s 625us/step - loss: 0.5189 - acc: 0.8352 - val_loss: 0.7412 - val_acc: 0.8621\n",
      "Epoch 132/600\n",
      "261/261 [==============================] - 0s 615us/step - loss: 0.5247 - acc: 0.8123 - val_loss: 0.7589 - val_acc: 0.7241\n",
      "Epoch 133/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.6854 - acc: 0.7356 - val_loss: 0.6901 - val_acc: 0.8276\n",
      "Epoch 134/600\n",
      "261/261 [==============================] - 0s 602us/step - loss: 0.5455 - acc: 0.8161 - val_loss: 0.6659 - val_acc: 0.7931\n",
      "Epoch 135/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.4957 - acc: 0.8659 - val_loss: 0.6808 - val_acc: 0.8621\n",
      "Epoch 136/600\n",
      "261/261 [==============================] - 0s 650us/step - loss: 0.5673 - acc: 0.8084 - val_loss: 0.7866 - val_acc: 0.7586\n",
      "Epoch 137/600\n",
      "261/261 [==============================] - 0s 659us/step - loss: 0.6757 - acc: 0.7548 - val_loss: 0.8183 - val_acc: 0.7586\n",
      "Epoch 138/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 0.5728 - acc: 0.8199 - val_loss: 0.6523 - val_acc: 0.8276\n",
      "Epoch 139/600\n",
      "261/261 [==============================] - 0s 650us/step - loss: 0.5098 - acc: 0.8352 - val_loss: 0.6722 - val_acc: 0.8621\n",
      "Epoch 140/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.5683 - acc: 0.7893 - val_loss: 0.7880 - val_acc: 0.8276\n",
      "Epoch 141/600\n",
      "261/261 [==============================] - 0s 636us/step - loss: 0.5209 - acc: 0.8352 - val_loss: 0.6547 - val_acc: 0.8621\n",
      "Epoch 142/600\n",
      "261/261 [==============================] - 0s 620us/step - loss: 0.6517 - acc: 0.7510 - val_loss: 1.1388 - val_acc: 0.6207\n",
      "Epoch 143/600\n",
      "261/261 [==============================] - 0s 676us/step - loss: 0.7691 - acc: 0.7126 - val_loss: 0.7271 - val_acc: 0.7931\n",
      "Epoch 144/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.5987 - acc: 0.8161 - val_loss: 0.6433 - val_acc: 0.8621\n",
      "Epoch 145/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 0.5280 - acc: 0.8161 - val_loss: 0.5920 - val_acc: 0.8276\n",
      "Epoch 146/600\n",
      "261/261 [==============================] - 0s 604us/step - loss: 0.4893 - acc: 0.8582 - val_loss: 0.6521 - val_acc: 0.8276\n",
      "Epoch 147/600\n",
      "261/261 [==============================] - 0s 637us/step - loss: 0.4836 - acc: 0.8582 - val_loss: 0.6139 - val_acc: 0.8276\n",
      "Epoch 148/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.4985 - acc: 0.8582 - val_loss: 0.6977 - val_acc: 0.7931\n",
      "Epoch 149/600\n",
      "261/261 [==============================] - 0s 661us/step - loss: 0.4983 - acc: 0.8467 - val_loss: 0.6790 - val_acc: 0.7931\n",
      "Epoch 150/600\n",
      "261/261 [==============================] - 0s 617us/step - loss: 0.4859 - acc: 0.8582 - val_loss: 0.6482 - val_acc: 0.8276\n",
      "Epoch 151/600\n",
      "261/261 [==============================] - 0s 636us/step - loss: 0.5079 - acc: 0.8314 - val_loss: 0.6680 - val_acc: 0.7931\n",
      "Epoch 152/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.5700 - acc: 0.8276 - val_loss: 0.6529 - val_acc: 0.7931\n",
      "Epoch 153/600\n",
      "261/261 [==============================] - 0s 629us/step - loss: 0.4852 - acc: 0.8621 - val_loss: 0.6533 - val_acc: 0.8276\n",
      "Epoch 154/600\n",
      "261/261 [==============================] - 0s 615us/step - loss: 0.5371 - acc: 0.8084 - val_loss: 0.6386 - val_acc: 0.8276\n",
      "Epoch 155/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.4934 - acc: 0.8314 - val_loss: 0.6340 - val_acc: 0.8276\n",
      "Epoch 156/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.4725 - acc: 0.8467 - val_loss: 0.6091 - val_acc: 0.8621\n",
      "Epoch 157/600\n",
      "261/261 [==============================] - 0s 625us/step - loss: 0.4859 - acc: 0.8276 - val_loss: 0.6123 - val_acc: 0.8276\n",
      "Epoch 158/600\n",
      "261/261 [==============================] - 0s 601us/step - loss: 0.4632 - acc: 0.8352 - val_loss: 0.6369 - val_acc: 0.8621\n",
      "Epoch 159/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.4613 - acc: 0.8659 - val_loss: 0.6854 - val_acc: 0.8276\n",
      "Epoch 160/600\n",
      "261/261 [==============================] - 0s 616us/step - loss: 0.4655 - acc: 0.8582 - val_loss: 0.7826 - val_acc: 0.7931\n",
      "Epoch 161/600\n",
      "261/261 [==============================] - 0s 599us/step - loss: 0.4514 - acc: 0.8506 - val_loss: 0.7189 - val_acc: 0.8621\n",
      "Epoch 162/600\n",
      "261/261 [==============================] - 0s 629us/step - loss: 0.4304 - acc: 0.8927 - val_loss: 0.5968 - val_acc: 0.8276\n",
      "Epoch 163/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.4308 - acc: 0.8774 - val_loss: 0.6242 - val_acc: 0.8276\n",
      "Epoch 164/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 0.4393 - acc: 0.8506 - val_loss: 0.6546 - val_acc: 0.8276\n",
      "Epoch 165/600\n",
      "261/261 [==============================] - 0s 617us/step - loss: 0.4625 - acc: 0.8506 - val_loss: 0.6413 - val_acc: 0.7931\n",
      "Epoch 166/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.4449 - acc: 0.8582 - val_loss: 0.6235 - val_acc: 0.8621\n",
      "Epoch 167/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.4779 - acc: 0.8391 - val_loss: 0.7048 - val_acc: 0.7241\n",
      "Epoch 168/600\n",
      "261/261 [==============================] - 0s 778us/step - loss: 0.4664 - acc: 0.8506 - val_loss: 0.6195 - val_acc: 0.7586\n",
      "Epoch 169/600\n",
      "261/261 [==============================] - 0s 656us/step - loss: 0.4284 - acc: 0.8851 - val_loss: 0.5907 - val_acc: 0.8276\n",
      "Epoch 170/600\n",
      "261/261 [==============================] - 0s 636us/step - loss: 0.4167 - acc: 0.8851 - val_loss: 0.5675 - val_acc: 0.8276\n",
      "Epoch 171/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.4061 - acc: 0.8927 - val_loss: 0.6187 - val_acc: 0.8621\n",
      "Epoch 172/600\n",
      "261/261 [==============================] - 0s 646us/step - loss: 0.4399 - acc: 0.8506 - val_loss: 0.5886 - val_acc: 0.8276\n",
      "Epoch 173/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.4240 - acc: 0.8659 - val_loss: 0.6765 - val_acc: 0.8621\n",
      "Epoch 174/600\n",
      "261/261 [==============================] - 0s 628us/step - loss: 0.4067 - acc: 0.8966 - val_loss: 0.6469 - val_acc: 0.7931\n",
      "Epoch 175/600\n",
      "261/261 [==============================] - 0s 688us/step - loss: 0.4377 - acc: 0.8506 - val_loss: 0.6204 - val_acc: 0.8276\n",
      "Epoch 176/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.4072 - acc: 0.8544 - val_loss: 0.5572 - val_acc: 0.8621\n",
      "Epoch 177/600\n",
      "261/261 [==============================] - 0s 627us/step - loss: 0.4538 - acc: 0.8276 - val_loss: 0.7732 - val_acc: 0.7586\n",
      "Epoch 178/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.6081 - acc: 0.8008 - val_loss: 0.8581 - val_acc: 0.6552\n",
      "Epoch 179/600\n",
      "261/261 [==============================] - 0s 621us/step - loss: 0.9741 - acc: 0.7165 - val_loss: 0.6677 - val_acc: 0.7931\n",
      "Epoch 180/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.5749 - acc: 0.8276 - val_loss: 0.7372 - val_acc: 0.7931\n",
      "Epoch 181/600\n",
      "261/261 [==============================] - 0s 623us/step - loss: 0.6005 - acc: 0.7739 - val_loss: 0.7410 - val_acc: 0.7931\n",
      "Epoch 182/600\n",
      "261/261 [==============================] - 0s 621us/step - loss: 0.4927 - acc: 0.8429 - val_loss: 0.6763 - val_acc: 0.7931\n",
      "Epoch 183/600\n",
      "261/261 [==============================] - 0s 641us/step - loss: 0.5007 - acc: 0.8506 - val_loss: 0.6012 - val_acc: 0.8276\n",
      "Epoch 184/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.4379 - acc: 0.8774 - val_loss: 0.5883 - val_acc: 0.8276\n",
      "Epoch 185/600\n",
      "261/261 [==============================] - 0s 610us/step - loss: 0.4583 - acc: 0.8506 - val_loss: 0.5906 - val_acc: 0.8276\n",
      "Epoch 186/600\n",
      "261/261 [==============================] - 0s 636us/step - loss: 0.4657 - acc: 0.8276 - val_loss: 0.9089 - val_acc: 0.6897\n",
      "Epoch 187/600\n",
      "261/261 [==============================] - 0s 616us/step - loss: 0.8529 - acc: 0.7548 - val_loss: 1.6848 - val_acc: 0.5172\n",
      "Epoch 188/600\n",
      "261/261 [==============================] - 0s 669us/step - loss: 0.8714 - acc: 0.7050 - val_loss: 0.7874 - val_acc: 0.7586\n",
      "Epoch 189/600\n",
      "261/261 [==============================] - 0s 614us/step - loss: 0.5513 - acc: 0.8467 - val_loss: 0.6893 - val_acc: 0.8276\n",
      "Epoch 190/600\n",
      "261/261 [==============================] - 0s 636us/step - loss: 0.4876 - acc: 0.8429 - val_loss: 0.6339 - val_acc: 0.8276\n",
      "Epoch 191/600\n",
      "261/261 [==============================] - 0s 666us/step - loss: 0.4973 - acc: 0.8314 - val_loss: 0.5926 - val_acc: 0.8966\n",
      "Epoch 192/600\n",
      "261/261 [==============================] - 0s 654us/step - loss: 0.4815 - acc: 0.8544 - val_loss: 0.7448 - val_acc: 0.7586\n",
      "Epoch 193/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.4465 - acc: 0.8659 - val_loss: 0.5871 - val_acc: 0.8621\n",
      "Epoch 194/600\n",
      "261/261 [==============================] - 0s 652us/step - loss: 0.4038 - acc: 0.9119 - val_loss: 0.6145 - val_acc: 0.8621\n",
      "Epoch 195/600\n",
      "261/261 [==============================] - 0s 629us/step - loss: 0.4391 - acc: 0.8659 - val_loss: 0.7949 - val_acc: 0.7931\n",
      "Epoch 196/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.7921 - acc: 0.7395 - val_loss: 1.1636 - val_acc: 0.6552\n",
      "Epoch 197/600\n",
      "261/261 [==============================] - 0s 665us/step - loss: 0.7073 - acc: 0.7625 - val_loss: 0.6378 - val_acc: 0.8276\n",
      "Epoch 198/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.5032 - acc: 0.8582 - val_loss: 0.5984 - val_acc: 0.8276\n",
      "Epoch 199/600\n",
      "261/261 [==============================] - 0s 620us/step - loss: 0.4988 - acc: 0.8199 - val_loss: 0.6921 - val_acc: 0.7931\n",
      "Epoch 200/600\n",
      "261/261 [==============================] - 0s 627us/step - loss: 0.5161 - acc: 0.8123 - val_loss: 0.5212 - val_acc: 0.8621\n",
      "Epoch 201/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.4965 - acc: 0.8199 - val_loss: 0.5716 - val_acc: 0.8276\n",
      "Epoch 202/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 0.4341 - acc: 0.8812 - val_loss: 0.6463 - val_acc: 0.7931\n",
      "Epoch 203/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.4355 - acc: 0.8429 - val_loss: 0.5578 - val_acc: 0.7931\n",
      "Epoch 204/600\n",
      "261/261 [==============================] - 0s 660us/step - loss: 0.4414 - acc: 0.8429 - val_loss: 0.5215 - val_acc: 0.8276\n",
      "Epoch 205/600\n",
      "261/261 [==============================] - 0s 592us/step - loss: 0.4387 - acc: 0.8506 - val_loss: 0.6659 - val_acc: 0.7931\n",
      "Epoch 206/600\n",
      "261/261 [==============================] - 0s 623us/step - loss: 0.4300 - acc: 0.8697 - val_loss: 0.6529 - val_acc: 0.7931\n",
      "Epoch 207/600\n",
      "261/261 [==============================] - 0s 610us/step - loss: 0.3994 - acc: 0.9004 - val_loss: 0.5899 - val_acc: 0.8621\n",
      "Epoch 208/600\n",
      "261/261 [==============================] - 0s 615us/step - loss: 0.3667 - acc: 0.9004 - val_loss: 0.5353 - val_acc: 0.8276\n",
      "Epoch 209/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.3742 - acc: 0.8889 - val_loss: 0.6047 - val_acc: 0.8621\n",
      "Epoch 210/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.5945 - acc: 0.7816 - val_loss: 0.7042 - val_acc: 0.6897\n",
      "Epoch 211/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.9436 - acc: 0.6973 - val_loss: 0.7627 - val_acc: 0.7586\n",
      "Epoch 212/600\n",
      "261/261 [==============================] - 0s 657us/step - loss: 0.5840 - acc: 0.7854 - val_loss: 0.6514 - val_acc: 0.8276\n",
      "Epoch 213/600\n",
      "261/261 [==============================] - 0s 628us/step - loss: 0.4813 - acc: 0.8506 - val_loss: 0.6405 - val_acc: 0.8621\n",
      "Epoch 214/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 0.4613 - acc: 0.8582 - val_loss: 0.6915 - val_acc: 0.7586\n",
      "Epoch 215/600\n",
      "261/261 [==============================] - 0s 610us/step - loss: 0.4707 - acc: 0.8352 - val_loss: 0.6025 - val_acc: 0.8621\n",
      "Epoch 216/600\n",
      "261/261 [==============================] - 0s 707us/step - loss: 0.4187 - acc: 0.8467 - val_loss: 0.5997 - val_acc: 0.8276\n",
      "Epoch 217/600\n",
      "261/261 [==============================] - 0s 624us/step - loss: 0.4501 - acc: 0.8391 - val_loss: 0.5655 - val_acc: 0.8621\n",
      "Epoch 218/600\n",
      "261/261 [==============================] - 0s 671us/step - loss: 0.4149 - acc: 0.8697 - val_loss: 0.5434 - val_acc: 0.8621\n",
      "Epoch 219/600\n",
      "261/261 [==============================] - 0s 627us/step - loss: 0.4685 - acc: 0.8659 - val_loss: 0.6635 - val_acc: 0.7931\n",
      "Epoch 220/600\n",
      "261/261 [==============================] - 0s 637us/step - loss: 0.7998 - acc: 0.7356 - val_loss: 0.6541 - val_acc: 0.7931\n",
      "Epoch 221/600\n",
      "261/261 [==============================] - 0s 628us/step - loss: 0.4802 - acc: 0.8429 - val_loss: 0.5224 - val_acc: 0.8966\n",
      "Epoch 222/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.5140 - acc: 0.8352 - val_loss: 0.6298 - val_acc: 0.8276\n",
      "Epoch 223/600\n",
      "261/261 [==============================] - 0s 617us/step - loss: 0.4214 - acc: 0.9004 - val_loss: 0.5701 - val_acc: 0.8966\n",
      "Epoch 224/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.4008 - acc: 0.8736 - val_loss: 0.5917 - val_acc: 0.8966\n",
      "Epoch 225/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.3749 - acc: 0.8966 - val_loss: 0.5086 - val_acc: 0.8966\n",
      "Epoch 226/600\n",
      "261/261 [==============================] - 0s 662us/step - loss: 0.3895 - acc: 0.8697 - val_loss: 0.5673 - val_acc: 0.8621\n",
      "Epoch 227/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 0.4025 - acc: 0.8697 - val_loss: 0.5426 - val_acc: 0.8621\n",
      "Epoch 228/600\n",
      "261/261 [==============================] - 0s 641us/step - loss: 0.3578 - acc: 0.9042 - val_loss: 0.5765 - val_acc: 0.8966\n",
      "Epoch 229/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.3663 - acc: 0.8812 - val_loss: 0.5788 - val_acc: 0.8966\n",
      "Epoch 230/600\n",
      "261/261 [==============================] - 0s 731us/step - loss: 0.5291 - acc: 0.8161 - val_loss: 0.6727 - val_acc: 0.7931\n",
      "Epoch 231/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 0.4652 - acc: 0.8544 - val_loss: 0.5064 - val_acc: 0.8621\n",
      "Epoch 232/600\n",
      "261/261 [==============================] - 0s 593us/step - loss: 0.4254 - acc: 0.8582 - val_loss: 0.5775 - val_acc: 0.8276\n",
      "Epoch 233/600\n",
      "261/261 [==============================] - 0s 599us/step - loss: 0.5637 - acc: 0.8161 - val_loss: 0.6902 - val_acc: 0.7931\n",
      "Epoch 234/600\n",
      "261/261 [==============================] - 0s 624us/step - loss: 0.4610 - acc: 0.8352 - val_loss: 0.6846 - val_acc: 0.8966\n",
      "Epoch 235/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.4107 - acc: 0.8812 - val_loss: 0.6572 - val_acc: 0.8621\n",
      "Epoch 236/600\n",
      "261/261 [==============================] - 0s 647us/step - loss: 0.4019 - acc: 0.8736 - val_loss: 0.6588 - val_acc: 0.8276\n",
      "Epoch 237/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.3683 - acc: 0.8966 - val_loss: 0.5706 - val_acc: 0.8621\n",
      "Epoch 238/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.4319 - acc: 0.8621 - val_loss: 0.5505 - val_acc: 0.8966\n",
      "Epoch 239/600\n",
      "261/261 [==============================] - 0s 583us/step - loss: 0.3884 - acc: 0.8812 - val_loss: 0.5472 - val_acc: 0.8621\n",
      "Epoch 240/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 0.3623 - acc: 0.9004 - val_loss: 0.5672 - val_acc: 0.8276\n",
      "Epoch 241/600\n",
      "261/261 [==============================] - 0s 608us/step - loss: 0.3794 - acc: 0.8774 - val_loss: 0.5208 - val_acc: 0.8621\n",
      "Epoch 242/600\n",
      "261/261 [==============================] - 0s 615us/step - loss: 0.3481 - acc: 0.9195 - val_loss: 0.5157 - val_acc: 0.8621\n",
      "Epoch 243/600\n",
      "261/261 [==============================] - 0s 594us/step - loss: 0.3350 - acc: 0.9272 - val_loss: 0.4912 - val_acc: 0.8621\n",
      "Epoch 244/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.5906 - acc: 0.8123 - val_loss: 0.6008 - val_acc: 0.8621\n",
      "Epoch 245/600\n",
      "261/261 [==============================] - 0s 594us/step - loss: 0.4379 - acc: 0.8697 - val_loss: 0.6187 - val_acc: 0.8276\n",
      "Epoch 246/600\n",
      "261/261 [==============================] - 0s 647us/step - loss: 0.7337 - acc: 0.7165 - val_loss: 0.5982 - val_acc: 0.8276\n",
      "Epoch 247/600\n",
      "261/261 [==============================] - 0s 612us/step - loss: 0.4508 - acc: 0.8736 - val_loss: 0.6550 - val_acc: 0.8276\n",
      "Epoch 248/600\n",
      "261/261 [==============================] - 0s 604us/step - loss: 0.3882 - acc: 0.8966 - val_loss: 0.4770 - val_acc: 0.8966\n",
      "Epoch 249/600\n",
      "261/261 [==============================] - 0s 615us/step - loss: 0.3800 - acc: 0.8889 - val_loss: 0.5330 - val_acc: 0.8621\n",
      "Epoch 250/600\n",
      "261/261 [==============================] - 0s 623us/step - loss: 0.4274 - acc: 0.8659 - val_loss: 0.5210 - val_acc: 0.8621\n",
      "Epoch 251/600\n",
      "261/261 [==============================] - 0s 645us/step - loss: 0.5285 - acc: 0.8238 - val_loss: 0.5950 - val_acc: 0.8621\n",
      "Epoch 252/600\n",
      "261/261 [==============================] - 0s 620us/step - loss: 0.4187 - acc: 0.8812 - val_loss: 0.6476 - val_acc: 0.8276\n",
      "Epoch 253/600\n",
      "261/261 [==============================] - 0s 610us/step - loss: 0.4472 - acc: 0.8506 - val_loss: 0.5328 - val_acc: 0.8621\n",
      "Epoch 254/600\n",
      "261/261 [==============================] - 0s 613us/step - loss: 0.3495 - acc: 0.9080 - val_loss: 0.5379 - val_acc: 0.8276\n",
      "Epoch 255/600\n",
      "261/261 [==============================] - 0s 602us/step - loss: 0.3882 - acc: 0.9080 - val_loss: 0.9970 - val_acc: 0.7241\n",
      "Epoch 256/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 1.2399 - acc: 0.6667 - val_loss: 1.4843 - val_acc: 0.5517\n",
      "Epoch 257/600\n",
      "261/261 [==============================] - 0s 636us/step - loss: 0.6207 - acc: 0.7739 - val_loss: 0.6724 - val_acc: 0.7241\n",
      "Epoch 258/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.4999 - acc: 0.8467 - val_loss: 0.4893 - val_acc: 0.8966\n",
      "Epoch 259/600\n",
      "261/261 [==============================] - 0s 603us/step - loss: 0.4298 - acc: 0.8736 - val_loss: 0.4755 - val_acc: 0.8621\n",
      "Epoch 260/600\n",
      "261/261 [==============================] - 0s 606us/step - loss: 0.3713 - acc: 0.8966 - val_loss: 0.5211 - val_acc: 0.8621\n",
      "Epoch 261/600\n",
      "261/261 [==============================] - 0s 616us/step - loss: 0.3663 - acc: 0.9004 - val_loss: 0.4945 - val_acc: 0.8621\n",
      "Epoch 262/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.3334 - acc: 0.9157 - val_loss: 0.5009 - val_acc: 0.8621\n",
      "Epoch 263/600\n",
      "261/261 [==============================] - 0s 608us/step - loss: 0.3285 - acc: 0.9234 - val_loss: 0.4957 - val_acc: 0.8621\n",
      "Epoch 264/600\n",
      "261/261 [==============================] - 0s 627us/step - loss: 0.3361 - acc: 0.9042 - val_loss: 0.6279 - val_acc: 0.8276\n",
      "Epoch 265/600\n",
      "261/261 [==============================] - 0s 633us/step - loss: 0.4744 - acc: 0.8467 - val_loss: 0.5514 - val_acc: 0.8276\n",
      "Epoch 266/600\n",
      "261/261 [==============================] - 0s 608us/step - loss: 0.4412 - acc: 0.8582 - val_loss: 0.5388 - val_acc: 0.8621\n",
      "Epoch 267/600\n",
      "261/261 [==============================] - 0s 657us/step - loss: 0.4269 - acc: 0.8582 - val_loss: 0.5568 - val_acc: 0.8621\n",
      "Epoch 268/600\n",
      "261/261 [==============================] - 0s 607us/step - loss: 0.4379 - acc: 0.8467 - val_loss: 0.5826 - val_acc: 0.8621\n",
      "Epoch 269/600\n",
      "261/261 [==============================] - 0s 625us/step - loss: 0.5274 - acc: 0.8429 - val_loss: 0.6234 - val_acc: 0.8276\n",
      "Epoch 270/600\n",
      "261/261 [==============================] - 0s 618us/step - loss: 0.4657 - acc: 0.8314 - val_loss: 0.5563 - val_acc: 0.8621\n",
      "Epoch 271/600\n",
      "261/261 [==============================] - 0s 616us/step - loss: 0.3829 - acc: 0.8736 - val_loss: 0.5712 - val_acc: 0.8621\n",
      "Epoch 272/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.3479 - acc: 0.9080 - val_loss: 0.5445 - val_acc: 0.8966\n",
      "Epoch 273/600\n",
      "261/261 [==============================] - 0s 600us/step - loss: 0.3753 - acc: 0.8697 - val_loss: 0.4896 - val_acc: 0.8621\n",
      "Epoch 274/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.3380 - acc: 0.9080 - val_loss: 0.4623 - val_acc: 0.8621\n",
      "Epoch 275/600\n",
      "261/261 [==============================] - 0s 623us/step - loss: 0.3243 - acc: 0.9195 - val_loss: 0.5023 - val_acc: 0.8621\n",
      "Epoch 276/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.3356 - acc: 0.9004 - val_loss: 0.4667 - val_acc: 0.8276\n",
      "Epoch 277/600\n",
      "261/261 [==============================] - 0s 645us/step - loss: 0.3234 - acc: 0.9195 - val_loss: 0.5196 - val_acc: 0.8621\n",
      "Epoch 278/600\n",
      "261/261 [==============================] - 0s 605us/step - loss: 0.3194 - acc: 0.9042 - val_loss: 0.4542 - val_acc: 0.8621\n",
      "Epoch 279/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 0.4243 - acc: 0.8467 - val_loss: 0.6065 - val_acc: 0.8276\n",
      "Epoch 280/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.7008 - acc: 0.7395 - val_loss: 0.8825 - val_acc: 0.6552\n",
      "Epoch 281/600\n",
      "261/261 [==============================] - 0s 662us/step - loss: 0.4725 - acc: 0.8123 - val_loss: 0.4730 - val_acc: 0.8621\n",
      "Epoch 282/600\n",
      "261/261 [==============================] - 0s 624us/step - loss: 0.3892 - acc: 0.9004 - val_loss: 0.4588 - val_acc: 0.8966\n",
      "Epoch 283/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.3781 - acc: 0.8697 - val_loss: 0.5655 - val_acc: 0.8621\n",
      "Epoch 284/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.3876 - acc: 0.8736 - val_loss: 0.4739 - val_acc: 0.8621\n",
      "Epoch 285/600\n",
      "261/261 [==============================] - 0s 641us/step - loss: 0.3240 - acc: 0.9234 - val_loss: 0.5099 - val_acc: 0.8621\n",
      "Epoch 286/600\n",
      "261/261 [==============================] - 0s 645us/step - loss: 0.3329 - acc: 0.9157 - val_loss: 0.6392 - val_acc: 0.8276\n",
      "Epoch 287/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.3597 - acc: 0.8851 - val_loss: 0.5169 - val_acc: 0.8621\n",
      "Epoch 288/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 0.3173 - acc: 0.9157 - val_loss: 0.4474 - val_acc: 0.8966\n",
      "Epoch 289/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.3110 - acc: 0.9080 - val_loss: 0.4948 - val_acc: 0.8966\n",
      "Epoch 290/600\n",
      "261/261 [==============================] - 0s 598us/step - loss: 0.4122 - acc: 0.8429 - val_loss: 0.5235 - val_acc: 0.8621\n",
      "Epoch 291/600\n",
      "261/261 [==============================] - 0s 637us/step - loss: 0.3806 - acc: 0.9080 - val_loss: 0.6172 - val_acc: 0.8276\n",
      "Epoch 292/600\n",
      "261/261 [==============================] - 0s 700us/step - loss: 0.3069 - acc: 0.9195 - val_loss: 0.5152 - val_acc: 0.8621\n",
      "Epoch 293/600\n",
      "261/261 [==============================] - 0s 695us/step - loss: 0.3992 - acc: 0.8697 - val_loss: 0.5561 - val_acc: 0.8621\n",
      "Epoch 294/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.4884 - acc: 0.8276 - val_loss: 0.5307 - val_acc: 0.8966\n",
      "Epoch 295/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 0.3790 - acc: 0.8582 - val_loss: 0.5981 - val_acc: 0.7931\n",
      "Epoch 296/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.3532 - acc: 0.8889 - val_loss: 0.4853 - val_acc: 0.8966\n",
      "Epoch 297/600\n",
      "261/261 [==============================] - 0s 623us/step - loss: 0.3146 - acc: 0.9272 - val_loss: 0.4985 - val_acc: 0.8621\n",
      "Epoch 298/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.3120 - acc: 0.9195 - val_loss: 0.5248 - val_acc: 0.8621\n",
      "Epoch 299/600\n",
      "261/261 [==============================] - 0s 637us/step - loss: 0.2970 - acc: 0.9272 - val_loss: 0.5069 - val_acc: 0.8621\n",
      "Epoch 300/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.3599 - acc: 0.8659 - val_loss: 0.5679 - val_acc: 0.8966\n",
      "Epoch 301/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 0.5183 - acc: 0.8429 - val_loss: 0.5725 - val_acc: 0.8276\n",
      "Epoch 302/600\n",
      "261/261 [==============================] - 0s 608us/step - loss: 0.4557 - acc: 0.8429 - val_loss: 0.5169 - val_acc: 0.8966\n",
      "Epoch 303/600\n",
      "261/261 [==============================] - 0s 616us/step - loss: 0.3660 - acc: 0.8966 - val_loss: 0.5326 - val_acc: 0.8621\n",
      "Epoch 304/600\n",
      "261/261 [==============================] - 0s 608us/step - loss: 0.3246 - acc: 0.9234 - val_loss: 0.4994 - val_acc: 0.8621\n",
      "Epoch 305/600\n",
      "261/261 [==============================] - 0s 615us/step - loss: 0.3137 - acc: 0.9119 - val_loss: 0.5284 - val_acc: 0.8621\n",
      "Epoch 306/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.3934 - acc: 0.8774 - val_loss: 0.5080 - val_acc: 0.8621\n",
      "Epoch 307/600\n",
      "261/261 [==============================] - 0s 674us/step - loss: 0.3403 - acc: 0.8812 - val_loss: 0.5038 - val_acc: 0.8966\n",
      "Epoch 308/600\n",
      "261/261 [==============================] - 0s 611us/step - loss: 0.3038 - acc: 0.9310 - val_loss: 0.4759 - val_acc: 0.8621\n",
      "Epoch 309/600\n",
      "261/261 [==============================] - 0s 604us/step - loss: 0.2919 - acc: 0.9387 - val_loss: 0.4701 - val_acc: 0.8621\n",
      "Epoch 310/600\n",
      "261/261 [==============================] - 0s 613us/step - loss: 0.2797 - acc: 0.9425 - val_loss: 0.5085 - val_acc: 0.9310\n",
      "Epoch 311/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 0.3028 - acc: 0.9119 - val_loss: 0.4479 - val_acc: 0.8621\n",
      "Epoch 312/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.3002 - acc: 0.9349 - val_loss: 0.4937 - val_acc: 0.8621\n",
      "Epoch 313/600\n",
      "261/261 [==============================] - 0s 633us/step - loss: 0.2852 - acc: 0.9310 - val_loss: 0.4565 - val_acc: 0.9310\n",
      "Epoch 314/600\n",
      "261/261 [==============================] - 0s 629us/step - loss: 0.3030 - acc: 0.9119 - val_loss: 0.4475 - val_acc: 0.8966\n",
      "Epoch 315/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.2793 - acc: 0.9464 - val_loss: 0.4491 - val_acc: 0.8966\n",
      "Epoch 316/600\n",
      "261/261 [==============================] - 0s 601us/step - loss: 0.2756 - acc: 0.9387 - val_loss: 0.4402 - val_acc: 0.9310\n",
      "Epoch 317/600\n",
      "261/261 [==============================] - 0s 581us/step - loss: 0.2849 - acc: 0.9387 - val_loss: 0.4642 - val_acc: 0.8966\n",
      "Epoch 318/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 0.3021 - acc: 0.9119 - val_loss: 0.4566 - val_acc: 0.8966\n",
      "Epoch 319/600\n",
      "261/261 [==============================] - 0s 620us/step - loss: 0.2827 - acc: 0.9349 - val_loss: 0.4860 - val_acc: 0.8966\n",
      "Epoch 320/600\n",
      "261/261 [==============================] - 0s 633us/step - loss: 0.3417 - acc: 0.8966 - val_loss: 0.4850 - val_acc: 0.8621\n",
      "Epoch 321/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 0.3178 - acc: 0.9042 - val_loss: 0.5695 - val_acc: 0.8621\n",
      "Epoch 322/600\n",
      "261/261 [==============================] - 0s 668us/step - loss: 0.7505 - acc: 0.7663 - val_loss: 1.1357 - val_acc: 0.6552\n",
      "Epoch 323/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 1.1260 - acc: 0.6628 - val_loss: 0.8875 - val_acc: 0.5862\n",
      "Epoch 324/600\n",
      "261/261 [==============================] - 0s 607us/step - loss: 0.5842 - acc: 0.8046 - val_loss: 0.6432 - val_acc: 0.7931\n",
      "Epoch 325/600\n",
      "261/261 [==============================] - 0s 662us/step - loss: 0.5200 - acc: 0.8123 - val_loss: 0.6030 - val_acc: 0.8276\n",
      "Epoch 326/600\n",
      "261/261 [==============================] - 0s 695us/step - loss: 0.6229 - acc: 0.7816 - val_loss: 0.6448 - val_acc: 0.8276\n",
      "Epoch 327/600\n",
      "261/261 [==============================] - 0s 606us/step - loss: 0.3914 - acc: 0.8851 - val_loss: 0.5450 - val_acc: 0.8621\n",
      "Epoch 328/600\n",
      "261/261 [==============================] - 0s 618us/step - loss: 0.3466 - acc: 0.8851 - val_loss: 0.5597 - val_acc: 0.8276\n",
      "Epoch 329/600\n",
      "261/261 [==============================] - 0s 611us/step - loss: 0.3842 - acc: 0.8774 - val_loss: 0.6845 - val_acc: 0.8276\n",
      "Epoch 330/600\n",
      "261/261 [==============================] - 0s 618us/step - loss: 0.3481 - acc: 0.9042 - val_loss: 0.4840 - val_acc: 0.8966\n",
      "Epoch 331/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 0.3198 - acc: 0.9310 - val_loss: 0.5443 - val_acc: 0.8966\n",
      "Epoch 332/600\n",
      "261/261 [==============================] - 0s 605us/step - loss: 0.3140 - acc: 0.9310 - val_loss: 0.4460 - val_acc: 0.8966\n",
      "Epoch 333/600\n",
      "261/261 [==============================] - 0s 624us/step - loss: 0.2997 - acc: 0.9195 - val_loss: 0.4615 - val_acc: 0.8966\n",
      "Epoch 334/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.3162 - acc: 0.9004 - val_loss: 0.4461 - val_acc: 0.8966\n",
      "Epoch 335/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.2912 - acc: 0.9272 - val_loss: 0.5114 - val_acc: 0.8621\n",
      "Epoch 336/600\n",
      "261/261 [==============================] - 0s 627us/step - loss: 0.3171 - acc: 0.9004 - val_loss: 0.4536 - val_acc: 0.9310\n",
      "Epoch 337/600\n",
      "261/261 [==============================] - 0s 604us/step - loss: 0.3621 - acc: 0.8927 - val_loss: 0.4481 - val_acc: 0.9310\n",
      "Epoch 338/600\n",
      "261/261 [==============================] - 0s 590us/step - loss: 0.3234 - acc: 0.9042 - val_loss: 0.4536 - val_acc: 0.8966\n",
      "Epoch 339/600\n",
      "261/261 [==============================] - 0s 621us/step - loss: 0.2936 - acc: 0.9272 - val_loss: 0.4557 - val_acc: 0.8966\n",
      "Epoch 340/600\n",
      "261/261 [==============================] - 0s 587us/step - loss: 0.3096 - acc: 0.9119 - val_loss: 0.6109 - val_acc: 0.8621\n",
      "Epoch 341/600\n",
      "261/261 [==============================] - 0s 620us/step - loss: 0.3679 - acc: 0.8736 - val_loss: 0.4629 - val_acc: 0.8966\n",
      "Epoch 342/600\n",
      "261/261 [==============================] - 0s 602us/step - loss: 0.3267 - acc: 0.9157 - val_loss: 0.4654 - val_acc: 0.8966\n",
      "Epoch 343/600\n",
      "261/261 [==============================] - 0s 624us/step - loss: 0.2923 - acc: 0.9272 - val_loss: 0.5251 - val_acc: 0.8966\n",
      "Epoch 344/600\n",
      "261/261 [==============================] - 0s 621us/step - loss: 0.2958 - acc: 0.9234 - val_loss: 0.4635 - val_acc: 0.9310\n",
      "Epoch 345/600\n",
      "261/261 [==============================] - 0s 598us/step - loss: 0.3512 - acc: 0.8851 - val_loss: 0.5204 - val_acc: 0.8621\n",
      "Epoch 346/600\n",
      "261/261 [==============================] - 0s 624us/step - loss: 0.3516 - acc: 0.9004 - val_loss: 0.4104 - val_acc: 0.9310\n",
      "Epoch 347/600\n",
      "261/261 [==============================] - 0s 581us/step - loss: 0.4243 - acc: 0.8544 - val_loss: 0.5073 - val_acc: 0.8621\n",
      "Epoch 348/600\n",
      "261/261 [==============================] - 0s 605us/step - loss: 0.3921 - acc: 0.8544 - val_loss: 0.4559 - val_acc: 0.8966\n",
      "Epoch 349/600\n",
      "261/261 [==============================] - 0s 655us/step - loss: 0.2902 - acc: 0.9349 - val_loss: 0.4040 - val_acc: 0.9310\n",
      "Epoch 350/600\n",
      "261/261 [==============================] - 0s 593us/step - loss: 0.2897 - acc: 0.9080 - val_loss: 0.4895 - val_acc: 0.8621\n",
      "Epoch 351/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.2760 - acc: 0.9425 - val_loss: 0.4132 - val_acc: 0.9310\n",
      "Epoch 352/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.2718 - acc: 0.9349 - val_loss: 0.4390 - val_acc: 0.9310\n",
      "Epoch 353/600\n",
      "261/261 [==============================] - 0s 610us/step - loss: 0.3003 - acc: 0.9195 - val_loss: 0.4079 - val_acc: 0.9310\n",
      "Epoch 354/600\n",
      "261/261 [==============================] - 0s 655us/step - loss: 0.3245 - acc: 0.9080 - val_loss: 0.4583 - val_acc: 0.9310\n",
      "Epoch 355/600\n",
      "261/261 [==============================] - 0s 695us/step - loss: 0.3042 - acc: 0.9157 - val_loss: 0.4371 - val_acc: 0.9310\n",
      "Epoch 356/600\n",
      "261/261 [==============================] - 0s 664us/step - loss: 0.2797 - acc: 0.9349 - val_loss: 0.4191 - val_acc: 0.8966\n",
      "Epoch 357/600\n",
      "261/261 [==============================] - 0s 752us/step - loss: 0.4219 - acc: 0.8582 - val_loss: 0.4385 - val_acc: 0.9310\n",
      "Epoch 358/600\n",
      "261/261 [==============================] - 0s 687us/step - loss: 0.3397 - acc: 0.8927 - val_loss: 0.4683 - val_acc: 0.9310\n",
      "Epoch 359/600\n",
      "261/261 [==============================] - 0s 653us/step - loss: 0.2701 - acc: 0.9540 - val_loss: 0.3818 - val_acc: 0.9310\n",
      "Epoch 360/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.2804 - acc: 0.9349 - val_loss: 0.4113 - val_acc: 0.9655\n",
      "Epoch 361/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.3344 - acc: 0.8966 - val_loss: 0.3985 - val_acc: 0.9310\n",
      "Epoch 362/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.2599 - acc: 0.9502 - val_loss: 0.4095 - val_acc: 0.9310\n",
      "Epoch 363/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.2529 - acc: 0.9349 - val_loss: 0.3906 - val_acc: 0.8966\n",
      "Epoch 364/600\n",
      "261/261 [==============================] - 0s 595us/step - loss: 0.3044 - acc: 0.9042 - val_loss: 0.4479 - val_acc: 0.8966\n",
      "Epoch 365/600\n",
      "261/261 [==============================] - 0s 646us/step - loss: 0.2676 - acc: 0.9387 - val_loss: 0.4267 - val_acc: 0.8966\n",
      "Epoch 366/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.2683 - acc: 0.9234 - val_loss: 0.4621 - val_acc: 0.8966\n",
      "Epoch 367/600\n",
      "261/261 [==============================] - 0s 612us/step - loss: 0.2546 - acc: 0.9464 - val_loss: 0.3960 - val_acc: 0.9310\n",
      "Epoch 368/600\n",
      "261/261 [==============================] - 0s 623us/step - loss: 0.2507 - acc: 0.9502 - val_loss: 0.3966 - val_acc: 0.9310\n",
      "Epoch 369/600\n",
      "261/261 [==============================] - 0s 612us/step - loss: 0.2704 - acc: 0.9272 - val_loss: 0.4068 - val_acc: 0.9310\n",
      "Epoch 370/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.2463 - acc: 0.9464 - val_loss: 0.4783 - val_acc: 0.9310\n",
      "Epoch 371/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.2644 - acc: 0.9195 - val_loss: 0.4527 - val_acc: 0.9310\n",
      "Epoch 372/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 0.2580 - acc: 0.9349 - val_loss: 0.4059 - val_acc: 0.9310\n",
      "Epoch 373/600\n",
      "261/261 [==============================] - 0s 744us/step - loss: 0.2387 - acc: 0.9540 - val_loss: 0.4263 - val_acc: 0.9310\n",
      "Epoch 374/600\n",
      "261/261 [==============================] - 0s 653us/step - loss: 0.2333 - acc: 0.9502 - val_loss: 0.3952 - val_acc: 0.9310\n",
      "Epoch 375/600\n",
      "261/261 [==============================] - 0s 654us/step - loss: 0.2343 - acc: 0.9464 - val_loss: 0.4243 - val_acc: 0.8966\n",
      "Epoch 376/600\n",
      "261/261 [==============================] - 0s 617us/step - loss: 0.3314 - acc: 0.8927 - val_loss: 0.7187 - val_acc: 0.7931\n",
      "Epoch 377/600\n",
      "261/261 [==============================] - 0s 657us/step - loss: 0.4422 - acc: 0.8582 - val_loss: 0.7926 - val_acc: 0.7586\n",
      "Epoch 378/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.3557 - acc: 0.8736 - val_loss: 0.5240 - val_acc: 0.8276\n",
      "Epoch 379/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.3358 - acc: 0.9004 - val_loss: 0.5159 - val_acc: 0.8621\n",
      "Epoch 380/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.3345 - acc: 0.8697 - val_loss: 0.4108 - val_acc: 0.8966\n",
      "Epoch 381/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.2908 - acc: 0.9234 - val_loss: 0.5049 - val_acc: 0.9310\n",
      "Epoch 382/600\n",
      "261/261 [==============================] - 0s 656us/step - loss: 0.3307 - acc: 0.8851 - val_loss: 0.4952 - val_acc: 0.9310\n",
      "Epoch 383/600\n",
      "261/261 [==============================] - 0s 614us/step - loss: 0.3684 - acc: 0.8659 - val_loss: 0.4586 - val_acc: 0.8621\n",
      "Epoch 384/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 0.3091 - acc: 0.8889 - val_loss: 0.5074 - val_acc: 0.8966\n",
      "Epoch 385/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.6430 - acc: 0.7854 - val_loss: 0.7569 - val_acc: 0.7586\n",
      "Epoch 386/600\n",
      "261/261 [==============================] - 0s 647us/step - loss: 0.4448 - acc: 0.8467 - val_loss: 0.7878 - val_acc: 0.7586\n",
      "Epoch 387/600\n",
      "261/261 [==============================] - 0s 608us/step - loss: 0.3202 - acc: 0.9080 - val_loss: 0.5102 - val_acc: 0.8966\n",
      "Epoch 388/600\n",
      "261/261 [==============================] - 0s 657us/step - loss: 0.2563 - acc: 0.9540 - val_loss: 0.5739 - val_acc: 0.8621\n",
      "Epoch 389/600\n",
      "261/261 [==============================] - 0s 600us/step - loss: 0.2740 - acc: 0.9195 - val_loss: 0.4868 - val_acc: 0.9310\n",
      "Epoch 390/600\n",
      "261/261 [==============================] - 0s 668us/step - loss: 0.2590 - acc: 0.9425 - val_loss: 0.5136 - val_acc: 0.9310\n",
      "Epoch 391/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.2416 - acc: 0.9502 - val_loss: 0.5795 - val_acc: 0.7931\n",
      "Epoch 392/600\n",
      "261/261 [==============================] - 0s 617us/step - loss: 0.2567 - acc: 0.9234 - val_loss: 0.4838 - val_acc: 0.9310\n",
      "Epoch 393/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.2522 - acc: 0.9234 - val_loss: 0.4502 - val_acc: 0.9310\n",
      "Epoch 394/600\n",
      "261/261 [==============================] - 0s 645us/step - loss: 0.2270 - acc: 0.9502 - val_loss: 0.4061 - val_acc: 0.9310\n",
      "Epoch 395/600\n",
      "261/261 [==============================] - 0s 615us/step - loss: 0.2304 - acc: 0.9425 - val_loss: 0.4549 - val_acc: 0.9310\n",
      "Epoch 396/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.2254 - acc: 0.9540 - val_loss: 0.4137 - val_acc: 0.9310\n",
      "Epoch 397/600\n",
      "261/261 [==============================] - 0s 659us/step - loss: 0.2180 - acc: 0.9540 - val_loss: 0.6685 - val_acc: 0.7931\n",
      "Epoch 398/600\n",
      "261/261 [==============================] - 0s 633us/step - loss: 0.8379 - acc: 0.7701 - val_loss: 0.7550 - val_acc: 0.7586\n",
      "Epoch 399/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.4261 - acc: 0.8314 - val_loss: 0.4818 - val_acc: 0.8966\n",
      "Epoch 400/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.2926 - acc: 0.9425 - val_loss: 0.5021 - val_acc: 0.8276\n",
      "Epoch 401/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 0.2757 - acc: 0.9272 - val_loss: 0.4302 - val_acc: 0.9310\n",
      "Epoch 402/600\n",
      "261/261 [==============================] - 0s 676us/step - loss: 0.2466 - acc: 0.9425 - val_loss: 0.4086 - val_acc: 0.9310\n",
      "Epoch 403/600\n",
      "261/261 [==============================] - 0s 646us/step - loss: 0.3806 - acc: 0.8582 - val_loss: 0.5236 - val_acc: 0.8276\n",
      "Epoch 404/600\n",
      "261/261 [==============================] - 0s 650us/step - loss: 0.3629 - acc: 0.9119 - val_loss: 0.4961 - val_acc: 0.9310\n",
      "Epoch 405/600\n",
      "261/261 [==============================] - 0s 656us/step - loss: 0.2976 - acc: 0.9195 - val_loss: 0.5087 - val_acc: 0.9310\n",
      "Epoch 406/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.2567 - acc: 0.9234 - val_loss: 0.4676 - val_acc: 0.8276\n",
      "Epoch 407/600\n",
      "261/261 [==============================] - 0s 627us/step - loss: 0.2387 - acc: 0.9425 - val_loss: 0.4147 - val_acc: 0.9310\n",
      "Epoch 408/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.2500 - acc: 0.9387 - val_loss: 0.4035 - val_acc: 0.9310\n",
      "Epoch 409/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 0.2679 - acc: 0.9349 - val_loss: 0.4966 - val_acc: 0.8621\n",
      "Epoch 410/600\n",
      "261/261 [==============================] - 0s 646us/step - loss: 0.2503 - acc: 0.9195 - val_loss: 0.4737 - val_acc: 0.9310\n",
      "Epoch 411/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 0.2369 - acc: 0.9464 - val_loss: 0.4525 - val_acc: 0.8966\n",
      "Epoch 412/600\n",
      "261/261 [==============================] - 0s 607us/step - loss: 0.2745 - acc: 0.9195 - val_loss: 0.4628 - val_acc: 0.9310\n",
      "Epoch 413/600\n",
      "261/261 [==============================] - 0s 624us/step - loss: 0.2300 - acc: 0.9502 - val_loss: 0.4692 - val_acc: 0.8966\n",
      "Epoch 414/600\n",
      "261/261 [==============================] - 0s 605us/step - loss: 0.2661 - acc: 0.9310 - val_loss: 0.4204 - val_acc: 0.9310\n",
      "Epoch 415/600\n",
      "261/261 [==============================] - 0s 711us/step - loss: 0.2269 - acc: 0.9540 - val_loss: 0.4113 - val_acc: 0.9310\n",
      "Epoch 416/600\n",
      "261/261 [==============================] - 0s 694us/step - loss: 0.2254 - acc: 0.9502 - val_loss: 0.4347 - val_acc: 0.8621\n",
      "Epoch 417/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.2415 - acc: 0.9310 - val_loss: 0.4537 - val_acc: 0.9310\n",
      "Epoch 418/600\n",
      "261/261 [==============================] - 0s 590us/step - loss: 0.2270 - acc: 0.9502 - val_loss: 0.4054 - val_acc: 0.8966\n",
      "Epoch 419/600\n",
      "261/261 [==============================] - 0s 603us/step - loss: 0.2162 - acc: 0.9464 - val_loss: 0.4198 - val_acc: 0.9310\n",
      "Epoch 420/600\n",
      "261/261 [==============================] - 0s 625us/step - loss: 0.2168 - acc: 0.9540 - val_loss: 0.4139 - val_acc: 0.9310\n",
      "Epoch 421/600\n",
      "261/261 [==============================] - 0s 636us/step - loss: 0.2245 - acc: 0.9349 - val_loss: 0.4369 - val_acc: 0.9310\n",
      "Epoch 422/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.2348 - acc: 0.9464 - val_loss: 0.3964 - val_acc: 0.8966\n",
      "Epoch 423/600\n",
      "261/261 [==============================] - 0s 685us/step - loss: 0.2141 - acc: 0.9502 - val_loss: 0.4372 - val_acc: 0.8966\n",
      "Epoch 424/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.2343 - acc: 0.9425 - val_loss: 0.5114 - val_acc: 0.8966\n",
      "Epoch 425/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.2262 - acc: 0.9349 - val_loss: 0.4177 - val_acc: 0.9310\n",
      "Epoch 426/600\n",
      "261/261 [==============================] - 0s 602us/step - loss: 0.2135 - acc: 0.9540 - val_loss: 0.4495 - val_acc: 0.8966\n",
      "Epoch 427/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.2310 - acc: 0.9425 - val_loss: 0.4056 - val_acc: 0.9310\n",
      "Epoch 428/600\n",
      "261/261 [==============================] - 0s 599us/step - loss: 0.2345 - acc: 0.9310 - val_loss: 0.3811 - val_acc: 0.9310\n",
      "Epoch 429/600\n",
      "261/261 [==============================] - 0s 612us/step - loss: 0.2583 - acc: 0.9349 - val_loss: 0.4728 - val_acc: 0.8621\n",
      "Epoch 430/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.2332 - acc: 0.9425 - val_loss: 0.4510 - val_acc: 0.9310\n",
      "Epoch 431/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.2005 - acc: 0.9579 - val_loss: 0.4647 - val_acc: 0.8966\n",
      "Epoch 432/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 0.2187 - acc: 0.9502 - val_loss: 0.4055 - val_acc: 0.9310\n",
      "Epoch 433/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 0.6718 - acc: 0.7854 - val_loss: 1.6055 - val_acc: 0.5172\n",
      "Epoch 434/600\n",
      "261/261 [==============================] - 0s 613us/step - loss: 0.5787 - acc: 0.7931 - val_loss: 0.5189 - val_acc: 0.8621\n",
      "Epoch 435/600\n",
      "261/261 [==============================] - 0s 600us/step - loss: 0.4070 - acc: 0.8429 - val_loss: 0.5598 - val_acc: 0.8276\n",
      "Epoch 436/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.3078 - acc: 0.9157 - val_loss: 0.4147 - val_acc: 0.8966\n",
      "Epoch 437/600\n",
      "261/261 [==============================] - 0s 610us/step - loss: 0.2430 - acc: 0.9310 - val_loss: 0.4704 - val_acc: 0.8621\n",
      "Epoch 438/600\n",
      "261/261 [==============================] - 0s 625us/step - loss: 0.2460 - acc: 0.9425 - val_loss: 0.4029 - val_acc: 0.9310\n",
      "Epoch 439/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 0.2333 - acc: 0.9387 - val_loss: 0.4742 - val_acc: 0.8966\n",
      "Epoch 440/600\n",
      "261/261 [==============================] - 0s 612us/step - loss: 0.2234 - acc: 0.9579 - val_loss: 0.4051 - val_acc: 0.8966\n",
      "Epoch 441/600\n",
      "261/261 [==============================] - 0s 654us/step - loss: 0.2182 - acc: 0.9464 - val_loss: 0.4520 - val_acc: 0.9310\n",
      "Epoch 442/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 0.2610 - acc: 0.9272 - val_loss: 0.5662 - val_acc: 0.8621\n",
      "Epoch 443/600\n",
      "261/261 [==============================] - 0s 653us/step - loss: 0.2734 - acc: 0.9004 - val_loss: 0.5151 - val_acc: 0.8621\n",
      "Epoch 444/600\n",
      "261/261 [==============================] - 0s 654us/step - loss: 0.2545 - acc: 0.9310 - val_loss: 0.4408 - val_acc: 0.8966\n",
      "Epoch 445/600\n",
      "261/261 [==============================] - 0s 652us/step - loss: 0.2472 - acc: 0.9425 - val_loss: 0.4579 - val_acc: 0.8621\n",
      "Epoch 446/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.2292 - acc: 0.9502 - val_loss: 0.3964 - val_acc: 0.9310\n",
      "Epoch 447/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.2294 - acc: 0.9387 - val_loss: 0.4410 - val_acc: 0.8966\n",
      "Epoch 448/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.2075 - acc: 0.9655 - val_loss: 0.4664 - val_acc: 0.8966\n",
      "Epoch 449/600\n",
      "261/261 [==============================] - 0s 651us/step - loss: 0.2092 - acc: 0.9464 - val_loss: 0.4320 - val_acc: 0.8966\n",
      "Epoch 450/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.1975 - acc: 0.9540 - val_loss: 0.4539 - val_acc: 0.8966\n",
      "Epoch 451/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.3361 - acc: 0.8697 - val_loss: 0.5466 - val_acc: 0.8621\n",
      "Epoch 452/600\n",
      "261/261 [==============================] - 0s 614us/step - loss: 0.2282 - acc: 0.9349 - val_loss: 0.4087 - val_acc: 0.9310\n",
      "Epoch 453/600\n",
      "261/261 [==============================] - 0s 604us/step - loss: 0.2048 - acc: 0.9617 - val_loss: 0.4853 - val_acc: 0.9310\n",
      "Epoch 454/600\n",
      "261/261 [==============================] - 0s 637us/step - loss: 0.2327 - acc: 0.9464 - val_loss: 0.3900 - val_acc: 0.8966\n",
      "Epoch 455/600\n",
      "261/261 [==============================] - 0s 597us/step - loss: 0.1986 - acc: 0.9732 - val_loss: 0.3741 - val_acc: 0.9310\n",
      "Epoch 456/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.1970 - acc: 0.9617 - val_loss: 0.4102 - val_acc: 0.8966\n",
      "Epoch 457/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.2162 - acc: 0.9464 - val_loss: 0.4528 - val_acc: 0.9310\n",
      "Epoch 458/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.1957 - acc: 0.9693 - val_loss: 0.4328 - val_acc: 0.9310\n",
      "Epoch 459/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.2067 - acc: 0.9693 - val_loss: 0.3995 - val_acc: 0.9310\n",
      "Epoch 460/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.2013 - acc: 0.9502 - val_loss: 0.5091 - val_acc: 0.8966\n",
      "Epoch 461/600\n",
      "261/261 [==============================] - 0s 655us/step - loss: 0.1845 - acc: 0.9770 - val_loss: 0.3734 - val_acc: 0.9310\n",
      "Epoch 462/600\n",
      "261/261 [==============================] - 0s 664us/step - loss: 0.1872 - acc: 0.9732 - val_loss: 0.4531 - val_acc: 0.8966\n",
      "Epoch 463/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 0.1976 - acc: 0.9655 - val_loss: 0.4282 - val_acc: 0.8966\n",
      "Epoch 464/600\n",
      "261/261 [==============================] - 0s 685us/step - loss: 0.2104 - acc: 0.9502 - val_loss: 0.4473 - val_acc: 0.9310\n",
      "Epoch 465/600\n",
      "261/261 [==============================] - 0s 628us/step - loss: 0.1892 - acc: 0.9617 - val_loss: 0.3926 - val_acc: 0.9310\n",
      "Epoch 466/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.1828 - acc: 0.9655 - val_loss: 0.4152 - val_acc: 0.8966\n",
      "Epoch 467/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.1822 - acc: 0.9770 - val_loss: 0.4117 - val_acc: 0.9310\n",
      "Epoch 468/600\n",
      "261/261 [==============================] - 0s 633us/step - loss: 0.1800 - acc: 0.9847 - val_loss: 0.3915 - val_acc: 0.9310\n",
      "Epoch 469/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.1805 - acc: 0.9732 - val_loss: 0.4038 - val_acc: 0.9310\n",
      "Epoch 470/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.1934 - acc: 0.9617 - val_loss: 0.4486 - val_acc: 0.9310\n",
      "Epoch 471/600\n",
      "261/261 [==============================] - 0s 609us/step - loss: 0.2131 - acc: 0.9425 - val_loss: 0.4712 - val_acc: 0.9310\n",
      "Epoch 472/600\n",
      "261/261 [==============================] - 0s 678us/step - loss: 0.2299 - acc: 0.9502 - val_loss: 0.4620 - val_acc: 0.9310\n",
      "Epoch 473/600\n",
      "261/261 [==============================] - 0s 626us/step - loss: 0.2423 - acc: 0.9349 - val_loss: 0.5072 - val_acc: 0.8966\n",
      "Epoch 474/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.2096 - acc: 0.9617 - val_loss: 0.3878 - val_acc: 0.9310\n",
      "Epoch 475/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.1967 - acc: 0.9579 - val_loss: 0.4119 - val_acc: 0.9310\n",
      "Epoch 476/600\n",
      "261/261 [==============================] - 0s 694us/step - loss: 0.1781 - acc: 0.9808 - val_loss: 0.4668 - val_acc: 0.8621\n",
      "Epoch 477/600\n",
      "261/261 [==============================] - 0s 726us/step - loss: 0.2213 - acc: 0.9579 - val_loss: 0.5351 - val_acc: 0.8966\n",
      "Epoch 478/600\n",
      "261/261 [==============================] - 0s 670us/step - loss: 0.2142 - acc: 0.9464 - val_loss: 0.4488 - val_acc: 0.8966\n",
      "Epoch 479/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.2241 - acc: 0.9464 - val_loss: 0.5101 - val_acc: 0.8621\n",
      "Epoch 480/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.2498 - acc: 0.9195 - val_loss: 0.5376 - val_acc: 0.8621\n",
      "Epoch 481/600\n",
      "261/261 [==============================] - 0s 685us/step - loss: 0.1977 - acc: 0.9655 - val_loss: 0.4113 - val_acc: 0.9310\n",
      "Epoch 482/600\n",
      "261/261 [==============================] - 0s 673us/step - loss: 0.2467 - acc: 0.9272 - val_loss: 0.4853 - val_acc: 0.8621\n",
      "Epoch 483/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.2239 - acc: 0.9349 - val_loss: 0.4398 - val_acc: 0.9310\n",
      "Epoch 484/600\n",
      "261/261 [==============================] - 0s 612us/step - loss: 0.2036 - acc: 0.9579 - val_loss: 0.4222 - val_acc: 0.8966\n",
      "Epoch 485/600\n",
      "261/261 [==============================] - 0s 657us/step - loss: 0.2376 - acc: 0.9195 - val_loss: 0.4806 - val_acc: 0.9310\n",
      "Epoch 486/600\n",
      "261/261 [==============================] - 0s 664us/step - loss: 0.2110 - acc: 0.9387 - val_loss: 0.4864 - val_acc: 0.9310\n",
      "Epoch 487/600\n",
      "261/261 [==============================] - 0s 693us/step - loss: 0.2113 - acc: 0.9502 - val_loss: 0.4559 - val_acc: 0.8966\n",
      "Epoch 488/600\n",
      "261/261 [==============================] - 0s 637us/step - loss: 0.2308 - acc: 0.9310 - val_loss: 0.5101 - val_acc: 0.8966\n",
      "Epoch 489/600\n",
      "261/261 [==============================] - 0s 650us/step - loss: 0.2463 - acc: 0.9234 - val_loss: 0.4475 - val_acc: 0.9310\n",
      "Epoch 490/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.1943 - acc: 0.9655 - val_loss: 0.4948 - val_acc: 0.8966\n",
      "Epoch 491/600\n",
      "261/261 [==============================] - 0s 668us/step - loss: 0.2196 - acc: 0.9387 - val_loss: 0.6062 - val_acc: 0.8276\n",
      "Epoch 492/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.2649 - acc: 0.9234 - val_loss: 0.4389 - val_acc: 0.9310\n",
      "Epoch 493/600\n",
      "261/261 [==============================] - 0s 667us/step - loss: 0.2094 - acc: 0.9617 - val_loss: 0.4374 - val_acc: 0.9310\n",
      "Epoch 494/600\n",
      "261/261 [==============================] - 0s 612us/step - loss: 0.2074 - acc: 0.9502 - val_loss: 0.3647 - val_acc: 0.9310\n",
      "Epoch 495/600\n",
      "261/261 [==============================] - 0s 629us/step - loss: 0.1988 - acc: 0.9655 - val_loss: 0.4972 - val_acc: 0.9310\n",
      "Epoch 496/600\n",
      "261/261 [==============================] - 0s 629us/step - loss: 0.1786 - acc: 0.9847 - val_loss: 0.4206 - val_acc: 0.9310\n",
      "Epoch 497/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.1848 - acc: 0.9732 - val_loss: 0.4531 - val_acc: 0.8966\n",
      "Epoch 498/600\n",
      "261/261 [==============================] - 0s 672us/step - loss: 0.1821 - acc: 0.9655 - val_loss: 0.5551 - val_acc: 0.8621\n",
      "Epoch 499/600\n",
      "261/261 [==============================] - 0s 627us/step - loss: 0.1767 - acc: 0.9655 - val_loss: 0.4936 - val_acc: 0.9310\n",
      "Epoch 500/600\n",
      "261/261 [==============================] - 0s 610us/step - loss: 0.2043 - acc: 0.9540 - val_loss: 0.5233 - val_acc: 0.9310\n",
      "Epoch 501/600\n",
      "261/261 [==============================] - 0s 639us/step - loss: 0.1809 - acc: 0.9770 - val_loss: 0.4218 - val_acc: 0.8966\n",
      "Epoch 502/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.2045 - acc: 0.9732 - val_loss: 0.4982 - val_acc: 0.8966\n",
      "Epoch 503/600\n",
      "261/261 [==============================] - 0s 662us/step - loss: 0.2247 - acc: 0.9387 - val_loss: 0.4919 - val_acc: 0.8966\n",
      "Epoch 504/600\n",
      "261/261 [==============================] - 0s 670us/step - loss: 0.1815 - acc: 0.9502 - val_loss: 0.5277 - val_acc: 0.9310\n",
      "Epoch 505/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.1761 - acc: 0.9808 - val_loss: 0.4498 - val_acc: 0.9310\n",
      "Epoch 506/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.1818 - acc: 0.9693 - val_loss: 0.4560 - val_acc: 0.8966\n",
      "Epoch 507/600\n",
      "261/261 [==============================] - 0s 633us/step - loss: 0.1936 - acc: 0.9579 - val_loss: 0.4210 - val_acc: 0.9310\n",
      "Epoch 508/600\n",
      "261/261 [==============================] - 0s 706us/step - loss: 0.2897 - acc: 0.8966 - val_loss: 0.5527 - val_acc: 0.8621\n",
      "Epoch 509/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.2719 - acc: 0.9195 - val_loss: 0.5778 - val_acc: 0.9310\n",
      "Epoch 510/600\n",
      "261/261 [==============================] - 0s 618us/step - loss: 0.2569 - acc: 0.9080 - val_loss: 0.3846 - val_acc: 0.9310\n",
      "Epoch 511/600\n",
      "261/261 [==============================] - 0s 610us/step - loss: 0.2152 - acc: 0.9349 - val_loss: 0.5009 - val_acc: 0.9310\n",
      "Epoch 512/600\n",
      "261/261 [==============================] - 0s 655us/step - loss: 0.1791 - acc: 0.9732 - val_loss: 0.3733 - val_acc: 0.9310\n",
      "Epoch 513/600\n",
      "261/261 [==============================] - 0s 596us/step - loss: 0.1769 - acc: 0.9693 - val_loss: 0.3879 - val_acc: 0.9310\n",
      "Epoch 514/600\n",
      "261/261 [==============================] - 0s 593us/step - loss: 0.1885 - acc: 0.9693 - val_loss: 0.4606 - val_acc: 0.8966\n",
      "Epoch 515/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.2196 - acc: 0.9502 - val_loss: 0.4156 - val_acc: 0.9310\n",
      "Epoch 516/600\n",
      "261/261 [==============================] - 0s 668us/step - loss: 0.1982 - acc: 0.9617 - val_loss: 0.5092 - val_acc: 0.8966\n",
      "Epoch 517/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.2216 - acc: 0.9425 - val_loss: 0.4430 - val_acc: 0.8966\n",
      "Epoch 518/600\n",
      "261/261 [==============================] - 0s 619us/step - loss: 0.3340 - acc: 0.9272 - val_loss: 0.4212 - val_acc: 0.8966\n",
      "Epoch 519/600\n",
      "261/261 [==============================] - 0s 628us/step - loss: 0.2192 - acc: 0.9310 - val_loss: 0.4539 - val_acc: 0.9310\n",
      "Epoch 520/600\n",
      "261/261 [==============================] - 0s 657us/step - loss: 0.1922 - acc: 0.9425 - val_loss: 0.5534 - val_acc: 0.8966\n",
      "Epoch 521/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.5462 - acc: 0.8391 - val_loss: 0.5768 - val_acc: 0.8276\n",
      "Epoch 522/600\n",
      "261/261 [==============================] - 0s 665us/step - loss: 0.3823 - acc: 0.8582 - val_loss: 0.5182 - val_acc: 0.9310\n",
      "Epoch 523/600\n",
      "261/261 [==============================] - 0s 682us/step - loss: 0.2385 - acc: 0.9349 - val_loss: 0.5044 - val_acc: 0.8621\n",
      "Epoch 524/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.2800 - acc: 0.9042 - val_loss: 0.9869 - val_acc: 0.7586\n",
      "Epoch 525/600\n",
      "261/261 [==============================] - 0s 652us/step - loss: 0.3138 - acc: 0.8812 - val_loss: 0.6435 - val_acc: 0.8276\n",
      "Epoch 526/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.2205 - acc: 0.9540 - val_loss: 0.6153 - val_acc: 0.8966\n",
      "Epoch 527/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 0.4552 - acc: 0.8697 - val_loss: 0.7281 - val_acc: 0.8276\n",
      "Epoch 528/600\n",
      "261/261 [==============================] - 0s 660us/step - loss: 1.7758 - acc: 0.6552 - val_loss: 1.1553 - val_acc: 0.6207\n",
      "Epoch 529/600\n",
      "261/261 [==============================] - 0s 647us/step - loss: 0.9091 - acc: 0.6820 - val_loss: 1.0230 - val_acc: 0.7241\n",
      "Epoch 530/600\n",
      "261/261 [==============================] - 0s 660us/step - loss: 0.5678 - acc: 0.8276 - val_loss: 0.7396 - val_acc: 0.8621\n",
      "Epoch 531/600\n",
      "261/261 [==============================] - 0s 638us/step - loss: 0.4817 - acc: 0.8659 - val_loss: 0.6405 - val_acc: 0.8276\n",
      "Epoch 532/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.4232 - acc: 0.8927 - val_loss: 0.5490 - val_acc: 0.8621\n",
      "Epoch 533/600\n",
      "261/261 [==============================] - 0s 655us/step - loss: 0.4520 - acc: 0.8314 - val_loss: 0.4738 - val_acc: 0.8621\n",
      "Epoch 534/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.3502 - acc: 0.8966 - val_loss: 0.4613 - val_acc: 0.8966\n",
      "Epoch 535/600\n",
      "261/261 [==============================] - 0s 714us/step - loss: 0.4025 - acc: 0.8621 - val_loss: 0.4717 - val_acc: 0.8621\n",
      "Epoch 536/600\n",
      "261/261 [==============================] - 0s 682us/step - loss: 0.3379 - acc: 0.9080 - val_loss: 0.4289 - val_acc: 0.8966\n",
      "Epoch 537/600\n",
      "261/261 [==============================] - 0s 771us/step - loss: 0.3117 - acc: 0.9272 - val_loss: 0.3993 - val_acc: 0.8966\n",
      "Epoch 538/600\n",
      "261/261 [==============================] - 0s 623us/step - loss: 0.3073 - acc: 0.9157 - val_loss: 0.4565 - val_acc: 0.8966\n",
      "Epoch 539/600\n",
      "261/261 [==============================] - 0s 666us/step - loss: 0.3154 - acc: 0.9080 - val_loss: 0.4401 - val_acc: 0.8621\n",
      "Epoch 540/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.3258 - acc: 0.9042 - val_loss: 0.4298 - val_acc: 0.8621\n",
      "Epoch 541/600\n",
      "261/261 [==============================] - 0s 649us/step - loss: 0.3305 - acc: 0.8966 - val_loss: 0.3781 - val_acc: 0.8966\n",
      "Epoch 542/600\n",
      "261/261 [==============================] - 0s 652us/step - loss: 0.2698 - acc: 0.9425 - val_loss: 0.4437 - val_acc: 0.8621\n",
      "Epoch 543/600\n",
      "261/261 [==============================] - 0s 655us/step - loss: 0.2841 - acc: 0.9080 - val_loss: 0.4344 - val_acc: 0.9310\n",
      "Epoch 544/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 0.2784 - acc: 0.9349 - val_loss: 0.4316 - val_acc: 0.8966\n",
      "Epoch 545/600\n",
      "261/261 [==============================] - 0s 647us/step - loss: 0.2713 - acc: 0.9425 - val_loss: 0.4486 - val_acc: 0.8966\n",
      "Epoch 546/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.2539 - acc: 0.9310 - val_loss: 0.4623 - val_acc: 0.8966\n",
      "Epoch 547/600\n",
      "261/261 [==============================] - 0s 608us/step - loss: 0.2555 - acc: 0.9349 - val_loss: 0.4138 - val_acc: 0.8966\n",
      "Epoch 548/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.2383 - acc: 0.9425 - val_loss: 0.4985 - val_acc: 0.8621\n",
      "Epoch 549/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.2566 - acc: 0.9425 - val_loss: 0.4468 - val_acc: 0.8966\n",
      "Epoch 550/600\n",
      "261/261 [==============================] - 0s 673us/step - loss: 0.2624 - acc: 0.9310 - val_loss: 0.4142 - val_acc: 0.8966\n",
      "Epoch 551/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.2521 - acc: 0.9234 - val_loss: 0.4334 - val_acc: 0.8621\n",
      "Epoch 552/600\n",
      "261/261 [==============================] - 0s 666us/step - loss: 0.2741 - acc: 0.9234 - val_loss: 0.5017 - val_acc: 0.8621\n",
      "Epoch 553/600\n",
      "261/261 [==============================] - 0s 679us/step - loss: 0.2835 - acc: 0.9042 - val_loss: 0.4123 - val_acc: 0.9310\n",
      "Epoch 554/600\n",
      "261/261 [==============================] - 0s 632us/step - loss: 0.2322 - acc: 0.9502 - val_loss: 0.3924 - val_acc: 0.9310\n",
      "Epoch 555/600\n",
      "261/261 [==============================] - 0s 654us/step - loss: 0.2174 - acc: 0.9579 - val_loss: 0.3923 - val_acc: 0.8966\n",
      "Epoch 556/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 0.2187 - acc: 0.9617 - val_loss: 0.4144 - val_acc: 0.8966\n",
      "Epoch 557/600\n",
      "261/261 [==============================] - 0s 671us/step - loss: 0.2351 - acc: 0.9349 - val_loss: 0.4177 - val_acc: 0.8966\n",
      "Epoch 558/600\n",
      "261/261 [==============================] - 0s 635us/step - loss: 0.2263 - acc: 0.9387 - val_loss: 0.4418 - val_acc: 0.8966\n",
      "Epoch 559/600\n",
      "261/261 [==============================] - 0s 662us/step - loss: 0.2620 - acc: 0.9157 - val_loss: 0.5566 - val_acc: 0.8621\n",
      "Epoch 560/600\n",
      "261/261 [==============================] - 0s 644us/step - loss: 0.3139 - acc: 0.9119 - val_loss: 0.4713 - val_acc: 0.8966\n",
      "Epoch 561/600\n",
      "261/261 [==============================] - 0s 646us/step - loss: 0.2526 - acc: 0.9349 - val_loss: 0.4200 - val_acc: 0.8966\n",
      "Epoch 562/600\n",
      "261/261 [==============================] - 0s 688us/step - loss: 0.2287 - acc: 0.9502 - val_loss: 0.4130 - val_acc: 0.9310\n",
      "Epoch 563/600\n",
      "261/261 [==============================] - 0s 670us/step - loss: 0.2206 - acc: 0.9502 - val_loss: 0.3890 - val_acc: 0.9310\n",
      "Epoch 564/600\n",
      "261/261 [==============================] - 0s 656us/step - loss: 0.2179 - acc: 0.9464 - val_loss: 0.3712 - val_acc: 0.8966\n",
      "Epoch 565/600\n",
      "261/261 [==============================] - 0s 628us/step - loss: 0.2172 - acc: 0.9540 - val_loss: 0.3676 - val_acc: 0.9310\n",
      "Epoch 566/600\n",
      "261/261 [==============================] - 0s 656us/step - loss: 0.2141 - acc: 0.9617 - val_loss: 0.3732 - val_acc: 0.9310\n",
      "Epoch 567/600\n",
      "261/261 [==============================] - 0s 676us/step - loss: 0.2025 - acc: 0.9540 - val_loss: 0.4099 - val_acc: 0.9310\n",
      "Epoch 568/600\n",
      "261/261 [==============================] - 0s 694us/step - loss: 0.2072 - acc: 0.9502 - val_loss: 0.3717 - val_acc: 0.9310\n",
      "Epoch 569/600\n",
      "261/261 [==============================] - 0s 611us/step - loss: 0.1994 - acc: 0.9693 - val_loss: 0.3973 - val_acc: 0.8966\n",
      "Epoch 570/600\n",
      "261/261 [==============================] - 0s 634us/step - loss: 0.2013 - acc: 0.9579 - val_loss: 0.4150 - val_acc: 0.9310\n",
      "Epoch 571/600\n",
      "261/261 [==============================] - 0s 618us/step - loss: 0.2515 - acc: 0.9195 - val_loss: 0.3797 - val_acc: 0.9310\n",
      "Epoch 572/600\n",
      "261/261 [==============================] - 0s 658us/step - loss: 0.1979 - acc: 0.9540 - val_loss: 0.3749 - val_acc: 0.9310\n",
      "Epoch 573/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.4091 - acc: 0.8774 - val_loss: 0.4647 - val_acc: 0.8966\n",
      "Epoch 574/600\n",
      "261/261 [==============================] - 0s 640us/step - loss: 0.3097 - acc: 0.9004 - val_loss: 0.5384 - val_acc: 0.7586\n",
      "Epoch 575/600\n",
      "261/261 [==============================] - 0s 662us/step - loss: 0.2538 - acc: 0.9272 - val_loss: 0.4130 - val_acc: 0.8966\n",
      "Epoch 576/600\n",
      "261/261 [==============================] - 0s 628us/step - loss: 0.2054 - acc: 0.9579 - val_loss: 0.4070 - val_acc: 0.9310\n",
      "Epoch 577/600\n",
      "261/261 [==============================] - 0s 614us/step - loss: 0.2302 - acc: 0.9310 - val_loss: 0.4516 - val_acc: 0.8966\n",
      "Epoch 578/600\n",
      "261/261 [==============================] - 0s 613us/step - loss: 0.1935 - acc: 0.9693 - val_loss: 0.3959 - val_acc: 0.9310\n",
      "Epoch 579/600\n",
      "261/261 [==============================] - 0s 630us/step - loss: 0.1901 - acc: 0.9693 - val_loss: 0.4409 - val_acc: 0.8966\n",
      "Epoch 580/600\n",
      "261/261 [==============================] - 0s 643us/step - loss: 0.2099 - acc: 0.9655 - val_loss: 0.3992 - val_acc: 0.9310\n",
      "Epoch 581/600\n",
      "261/261 [==============================] - 0s 646us/step - loss: 0.2155 - acc: 0.9387 - val_loss: 0.4317 - val_acc: 0.8621\n",
      "Epoch 582/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.2934 - acc: 0.8927 - val_loss: 0.3753 - val_acc: 0.9655\n",
      "Epoch 583/600\n",
      "261/261 [==============================] - 0s 642us/step - loss: 0.2373 - acc: 0.9080 - val_loss: 0.4662 - val_acc: 0.9310\n",
      "Epoch 584/600\n",
      "261/261 [==============================] - 0s 607us/step - loss: 0.2320 - acc: 0.9502 - val_loss: 0.4229 - val_acc: 0.8621\n",
      "Epoch 585/600\n",
      "261/261 [==============================] - 0s 641us/step - loss: 0.2048 - acc: 0.9617 - val_loss: 0.4339 - val_acc: 0.8966\n",
      "Epoch 586/600\n",
      "261/261 [==============================] - 0s 618us/step - loss: 0.2100 - acc: 0.9464 - val_loss: 0.5258 - val_acc: 0.8966\n",
      "Epoch 587/600\n",
      "261/261 [==============================] - 0s 611us/step - loss: 0.2592 - acc: 0.9157 - val_loss: 0.5002 - val_acc: 0.8966\n",
      "Epoch 588/600\n",
      "261/261 [==============================] - 0s 645us/step - loss: 0.1978 - acc: 0.9540 - val_loss: 0.5055 - val_acc: 0.7931\n",
      "Epoch 589/600\n",
      "261/261 [==============================] - 0s 656us/step - loss: 0.2744 - acc: 0.9272 - val_loss: 0.6327 - val_acc: 0.8621\n",
      "Epoch 590/600\n",
      "261/261 [==============================] - 0s 625us/step - loss: 0.2136 - acc: 0.9655 - val_loss: 0.3977 - val_acc: 0.9310\n",
      "Epoch 591/600\n",
      "261/261 [==============================] - 0s 674us/step - loss: 0.2155 - acc: 0.9655 - val_loss: 0.4834 - val_acc: 0.8966\n",
      "Epoch 592/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.1919 - acc: 0.9655 - val_loss: 0.4194 - val_acc: 0.8966\n",
      "Epoch 593/600\n",
      "261/261 [==============================] - 0s 650us/step - loss: 0.1928 - acc: 0.9617 - val_loss: 0.3929 - val_acc: 0.9310\n",
      "Epoch 594/600\n",
      "261/261 [==============================] - 0s 631us/step - loss: 0.1764 - acc: 0.9732 - val_loss: 0.3898 - val_acc: 0.9310\n",
      "Epoch 595/600\n",
      "261/261 [==============================] - 0s 618us/step - loss: 0.1785 - acc: 0.9732 - val_loss: 0.3666 - val_acc: 0.9310\n",
      "Epoch 596/600\n",
      "261/261 [==============================] - 0s 648us/step - loss: 0.1852 - acc: 0.9617 - val_loss: 0.4031 - val_acc: 0.9310\n",
      "Epoch 597/600\n",
      "261/261 [==============================] - 0s 655us/step - loss: 0.1727 - acc: 0.9847 - val_loss: 0.3637 - val_acc: 0.8966\n",
      "Epoch 598/600\n",
      "261/261 [==============================] - 0s 717us/step - loss: 0.1782 - acc: 0.9808 - val_loss: 0.3704 - val_acc: 0.9310\n",
      "Epoch 599/600\n",
      "261/261 [==============================] - 0s 622us/step - loss: 0.1706 - acc: 0.9770 - val_loss: 0.3775 - val_acc: 0.8966\n",
      "Epoch 600/600\n",
      "261/261 [==============================] - 0s 598us/step - loss: 0.1690 - acc: 0.9770 - val_loss: 0.4204 - val_acc: 0.9310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fec1d0165f8>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#128 - 1 hidden, epochs 30 - 75%\n",
    "#256 - 1 hidden, epochs 80 - 72%\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "                   \n",
    "model.fit(X_train, y_train,epochs=600, batch_size=20, verbose=1, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXrMbsaQn82k"
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "rMqZBJbY4XU2",
    "outputId": "85ba4a73-6bcb-4e08-a7ef-85aa3a23eba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 162us/step\n",
      "Loss : 0.47, Accuracy : 87.20%\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(X_test, y_test)\n",
    "print(\"Loss : {0:.2f}, Accuracy : {1:.2f}%\".format(evaluation[0], evaluation[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Za3cB64bFnNc"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0vyi17jHGGsp",
    "outputId": "67872fb2-1c8c-4ac7-c869-c52df8413dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Set : 85.60%\n"
     ]
    }
   ],
   "source": [
    "ascore = accuracy_score(np.argmax(np.round(predictions),axis=1),np.argmax(np.round(y_test),axis=1))\n",
    "print(\"Accuracy on Test Set : {0:.2f}%\".format(ascore*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWq7W4ChoPLn"
   },
   "source": [
    "## Save the model\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isTta0RpIW-h"
   },
   "outputs": [],
   "source": [
    "pickle.dump(model, open('neural_network_model','wb'))\n",
    "files.download('neural_network_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aKV-j5LoWXx"
   },
   "source": [
    "# Viz - Looking at Validation & Train Loss\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "gkg_sz3W4wu-",
    "outputId": "86b11fe0-f154-49f2-bd5d-34f18063de4a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2deZgU1dXG3zMbw8g+ICLLgIrsDAzD\noiCCiKKJGFfEJiKKRAiixuUzjp8Y/UaTuBuJAsYttCLRqKioiYASYzAgIgqIIA44iCzDKoPCwPn+\nuFVT1dVV3dU91Uv1nN/z9NO13Kq6t5a3bp177rnEzBAEQRD8T1aqMyAIgiB4gwi6IAhChiCCLgiC\nkCGIoAuCIGQIIuiCIAgZggi6IAhChiCCnsEQUTYR/UBEHbxMm0qI6CQi8tzXlojOJKIK0/w6IjrN\nTdo4jvUUEd0e7/aC4EROqjMgGBDRD6bZAgA/ATiizf+KmYOx7I+ZjwBo5HXa+gAzd/FiP0Q0EcA4\nZh5m2vdEL/YtCFZE0NMIZq4VVK0GOJGZ33NKT0Q5zFyTjLwJQjTkfkw9YnLxEUT0f0T0EhG9SET7\nAYwjolOIaCkR7SGirUT0GBHlaulziIiJqKM2P0db/zYR7Sei/xBRp1jTauvPIaKviGgvEf2JiP5N\nRFc65NtNHn9FRBuIaDcRPWbaNpuIHiaiKiLaCGBUhPNTRkRzLctmENFD2vREIlqrledrrfbstK9K\nIhqmTRcQ0V+1vK0G0M+S9g4i2qjtdzURjdaW9wLwOIDTNHPWTtO5vcu0/bVa2auI6DUiauPm3MRy\nnvX8ENF7RLSLiL4noltNx/lf7ZzsI6LlRHS8nXmLiD7Ur7N2Ppdox9kF4A4i6kxEi7Vj7NTOW1PT\n9kVaGXdo6x8lonwtz91M6doQUTURFTqVV7CBmeWXhj8AFQDOtCz7PwCHAJwH9TJuCKA/gIFQX1sn\nAPgKwFQtfQ4ABtBRm58DYCeAUgC5AF4CMCeOtMcC2A/gfG3dbwAcBnClQ1nc5PF1AE0BdASwSy87\ngKkAVgNoB6AQwBJ129oe5wQAPwA4xrTv7QBKtfnztDQE4AwABwH01tadCaDCtK9KAMO06QcAvA+g\nOYAiAGssaS8F0Ea7JpdreWitrZsI4H1LPucAuEubPkvLYx8A+QD+DGCRm3MT43luCmAbgOsBNADQ\nBMAAbd1vAXwGoLNWhj4AWgA4yXquAXyoX2etbDUAJgPIhrofTwYwAkCedp/8G8ADpvJ8oZ3PY7T0\ng7V1swCUm45zE4BXU/0c+u2X8gzIz+HCOAv6oijb3Qzgb9q0nUg/aUo7GsAXcaS9CsC/TOsIwFY4\nCLrLPA4yrf87gJu16SVQpid93blWkbHseymAy7XpcwCsi5D2TQC/1qYjCfpm87UAMMWc1ma/XwD4\nmTYdTdCfA3CvaV0TqHaTdtHOTYzn+ZcAljmk+1rPr2W5G0HfGCUPF+vHBXAagO8BZNukGwzgGwCk\nza8EcKHXz1Wm/8Tk4j++Nc8QUVciekv7hN4H4G4ALSNs/71puhqRG0Kd0h5vzgerJ7DSaScu8+jq\nWAA2RcgvALwAYKw2fbk2r+fj50T0sWYO2ANVO450rnTaRMoDEV1JRJ9pZoM9ALq63C+gyle7P2be\nB2A3gLamNK6uWZTz3B5KuO2ItC4a1vvxOCKaR0RbtDw8a8lDBasG+BCY+d9Qtf0hRNQTQAcAb8WZ\np3qLCLr/sLrszYSqEZ7EzE0A3AlVY04kW6FqkAAAIiKECpCVuuRxK5QQ6ERzq5wH4EwiagtlEnpB\ny2NDAC8DuA/KHNIMwD9c5uN7pzwQ0QkAnoAyOxRq+/3StN9oLpbfQZlx9P01hjLtbHGRLyuRzvO3\nAE502M5p3QEtTwWmZcdZ0ljL9wco76xeWh6utOShiIiyHfLxPIBxUF8T85j5J4d0ggMi6P6nMYC9\nAA5ojUq/SsIx3wRQQkTnEVEOlF22VYLyOA/ADUTUVmsg+59IiZn5eyizwLNQ5pb12qoGUHbdHQCO\nENHPoWy9bvNwOxE1I+WnP9W0rhGUqO2AerddA1VD19kGoJ25cdLCiwCuJqLeRNQA6oXzL2Z2/OKJ\nQKTzPB9AByKaSkQNiKgJEQ3Q1j0F4P+I6ERS9CGiFlAvsu+hGt+ziWgSTC+fCHk4AGAvEbWHMvvo\n/AdAFYB7STU0NySiwab1f4Uy0VwOJe5CjIig+5+bAIyHaqScCdV4mVCYeRuAMQAegnpATwTwKVTN\nzOs8PgFgIYDPASyDqmVH4wUom3ituYWZ9wC4EcCrUA2LF0O9mNwwHepLoQLA2zCJDTOvAvAnAP/V\n0nQB8LFp238CWA9gGxGZTSf69u9AmUZe1bbvACDgMl9WHM8zM+8FMBLARVAvma8AnK6tvh/Aa1Dn\neR9UA2W+Zkq7BsDtUA3kJ1nKZsd0AAOgXizzAbxiykMNgJ8D6AZVW98MdR309RVQ1/knZv4oxrIL\nMBogBCFutE/o7wBczMz/SnV+BP9CRM9DNbTeleq8+BHpWCTEBRGNgvIoOQjl9nYYqpYqCHGhtUec\nD6BXqvPiV8TkIsTLEAAboWzHZwO4QBqxhHghovugfOHvZebNqc6PXxGTiyAIQoYgNXRBEIQMIWU2\n9JYtW3LHjh1TdXhBEARf8sknn+xkZls34ZQJeseOHbF8+fJUHV4QBMGXEJFjb2kxuQiCIGQIIuiC\nIAgZQlRBJ6KniWg7EX3hsJ60eMgbiGgVEZV4n01BEAQhGm5q6M8iwqACUCFKO2u/SVBdtQVBEIQk\nE1XQmXkJVOwLJ84H8DwrlgJoRtqIK4IgCELy8MKG3hahMZEr4RBKlYgmacNbLd+xY4cHhxYEId0J\nBoGOHYGsLPUfdDnUebzbpSPJKktSG0WZeRYzlzJzaatWkaKtCoKQCQSDwKRJwKZNALP6nzABaNky\nsrjFu106YleWX/4SmDLF+2N5IehbEBr8vx3iC84vCEIGYK6NXnEFUF0duv7wYaCqyhC3SZPCxbms\nLL7t0hG7sjADTzzhvah7IejzAVyhebsMArCXmbd6sF9BEHyCLuJEqvap10aPHo2+bXW1Ej0zm12E\n56quBsaPT/8a+6YIgyY+8YS3+Xbjtvgi1EgjXYiokoiuJqJriehaLckCqKh7GwDMhhpAVxCEeoLZ\npAAoIY8Vq4C3aOFuuyNHjBr7uHGJMWOYsdrCp0xRZiAi9WvZMlSg3eTH+jKrCymLtlhaWsrS9V8Q\n/EkwqIRo82YlbkfChn2Ojexs4Lnn1HRZWeRabTTmzAEC8Y75FAH9xWU1n1jRy/Lvf6saeDSI3H3J\nGOnpE2YutV0ngi4IQiy4FbZYyc1V4nboUN32U1gI7NwZ//bml1WHDkB5uVo+fnzdX1x2FBUBFRXu\n00cSdOn6LwgpJhhUDzWRMjWYP9mjubvFagKwbkcE5OQYaXUvEvO0vl/9OOPHey/mgGr0rKuYA6rh\n1JzfWF0lr7oq1CNl/HjgyisTI+Z5ecYLwxOYOSW/fv36sSDUd+bMYS4oYFbyYfyOOUb9rMuJmCdP\njryt9ZeXp9LqTJ4cfRu//4hC5wsKQs+BE40aJS+PWVnu8mQFwHJ20NWECXa0nwi64DfmzGEuKlJi\nUVQU38NopagoPjHQ8+I2fVGROl59EPNo5yDS9U1mfojiu2ciCbqYXAQhAk7ueHXxgzabSeJt/Lv+\neneufTqbNgGNGrlrpMtUzOfLfF2zstT/uHHJzU+HDt7vUwRdEBww21MBJeRmqquVsMa6T3OvwXip\nqopdEA4ciP94mYB+vrxws6wrBQUe2841RNAFwYHrr4/eSKc3wMWyT68aFBMhCJmKWUDtem4mgyxN\nbYuKgFmzEuNaKYIuCA5UVblL9+STqtY3ZYrhMZKTEy70waD7fUajsDAxgpCJEClPFf18xWKq8orC\nQqMTVEVF4q6dCLqQliQ70l4wGOru16iR+22ZgV/9Stmndde2I0fUvG6fzcnx1kb76KPqv6jIu30m\ngqw0UBhmYMECNR0MepOnwkJgxAh1baNBBFx6qZGXWDoRxYxTa2mif+LlIjgxZ45ytTN7BFhd78xp\nnTxP5sxhLiw09lFYqLw89PSFhaHr/fZr2pR5xIjU58Pp1749c5Mmqc8HoK63WzfPZPyefDL+5wMR\nvFykp6iQdrRsaW+asPYA1BstzXbuvDzg6afV9IQJqrOKkHqyshJcM42C/iVTl5ACXvLXv8b/xSY9\nRQVPSbQ5xMnObF1u12h56JBaXlaWPDEvKEgP00I606MH0LBh3faRnx/fdnqDaLqIOQA0a5aY/cpt\nKMSEXbD+ZMalNjc8RhL+ZD28eoPbr36VnOOlI8ccEz1N9+7A7Nl1O87JJ8e+jdmjJJ1euiLoQlpg\n5/JlF8/aDU41/cJC+/R5eaENj+kAsxIMwF0DWToTTw168GDgj3+Mni43t+6eHbEEsLISDKbW5GNF\nBF1IC5xcvvTlblz3ovW81D0CzOTmAjU1nhbFM3SPlhQ1R3lCu3bAwYOxb9e+PfDjj6HLmjRxt22s\nL5B9+2JLDxj3VawdwLzGai4SQRcSjhvbuFPvxA4dlHjbue7poh6th151tRJ5u+7pDRokr4aVjjXt\nWOzHREowWrZ0v03HjjFnCQDw8svA0qXhx3fDww9HT9O4cex5slJd7Z3/f7z5+dnPQudF0AXPsBPu\nKVPcxSopL1eNTGb0Rifd9GBFX+6mh55TLfeHH6KVyhsKC9WXRTqh+8a7oVEjYOxYJeZ2XzpORPJn\nb9fOeV1NDfCPf7g/jpnzzou8fvZsYNq0+PadKG68Mb7tTj89dN5Nu0M8iKDXM+waNceNszcZ2NnG\nAwHg2GNVDYMotNHJybatL09FD71Y2bUr/VwdW7Rwbw4ZMEC9kDZsAP78Z/fHaNXKed3dd0fedu/e\n0Hm7l4Pd10LbtpH3m5eXuJpsvIweHd92zZuHzifqKzDN6iJCook1joWdCOuNU/oLQK/xRyIYVGaZ\nZHif5ObGL8rpZge3fg1Fo1s3d+JfXm68rO+9F/jFL9R0fr76Gnr8cSPtLbe4O/bvf6+++qZOBR57\nTL0cBw8GNm4ErrlGpVm5Erj/fntz3qJFqkfnAw+o+dxcQwhbt1ZfkPo6O6ZNA/70p8RcwzffVPdu\nSYmq0Gzf7n7bKVOAc85RZX/rLeXxkzCcehwl+ic9RVODNfB/tF9hYXhPTH0ds/ved9ZtE/XTBw1I\n1P4bN058GfRfdrYqSyzX7PrrmSdNipzm3HPVtdPnN28OvUdWrgxNf/zxxrnNyQld16BB6P3ghtdf\nD8/T8ccb6/VlL7+sfgDzoEFqnVMMeD3W+ejR3p3/li2N6aNHjfzZ9WSO9Dt82P25cQMkHrqgE2vI\n1f37Q80zek1Lx22Nf/PmxAeTys1V/uBmM1Fhofq8dXKFjBWi2GvNkXCyXROpgYYDgdiuWU5O7G0A\n1gZX63yDBur/6FHgrLNC18UT8dFtA29urmFy0bcpLw/3jjFHUoy385GVlSuBF1805q0mEmb3+0pm\nm4wIej2jvDy2BjZrT0zr57xbu3iLFvF7UriBCBg6VImg2axTVaWOHUsDYST27VNtBl6IeoMG9o3M\nRMC11xovwPJyNZK8GxIh6Hl5xnTfvqHrzHl322u4LoIeCIR2ULKGoo1H0O0affPzne33sfRC1l+G\nyUIE3cfE2wXfTe1Ct6VGgsjdvnJzgT17Ems/Zwbef9/+a6GqyruRetq0UeJx5ZWq1j95cvz76tRJ\n7WvWLGOQ6KIiFefD3KAZCLj32XYj6NZr5raGbrfuppuMabe9hu1E1+4+MjeKmrcxf+lZQ9HGI6Dr\n1tnn0UnQY2ncjyVqpxeIoPuUWLrgWzvzuOHHH71pic/OVg9mMnp3JuMYuoA1a6ZeUjNmxP/Qtm6t\n/gMBJUxHjzrHynbrthlPDd2aPlIN3brO+sXmptdwXWro0YjHvGH3Eogk6LGYwLwyAblFBN2nROqC\nP2WKEYdbHysxnuG2mOsu6keOZNbQZ2PGqP9mzYyyPflk/CYYt19Zbju0xCPo1mtsFaHcXOd1dkSr\nwbo1H+XmAk2buj8uoNw1Y8UuP/n5xrGtOPXFsEMEXXCFk/li0yZvu6Ezh5oD6jv6A6q70z3yiKpR\nDxpkpDGfJ3Pt1sqHHxpjlkb6ygoG3buauhH03bsjr7fm+T//MabdmH6i1WDt7s0WLezzkZOjXmZu\nhDEYBBYvjp7ODusLMz8/9EVmRjeT6eeiUSPnTnXJFnTxQ09zgkFV6968WT0o5eXqhqqLr3Ws7Nxp\nCE5945hj1BfOkSOqI4zuLaN35V6zRv0vWqT+mzRRZpN//EOds++/V3FErJ1vALVPq5lI/8oym13K\nytybk9wIemWl+l+xAli9Onw9ETBzpirHffcZyz/9VPlgT55s+Ijff3/oy8bN4Mc9eqiAXt26Kf/u\nhg3te2Dqgvr440Dv3qHrXnkFOO640GVlZfHH+/nvf4EPPlCN3keOGGaYZ58F+vQJTx8IqDy8+qp6\nKQcCqnPWoUPqvO7erV72yRZ0W1/GZPzEDz06dj7eBQVq1J3sbO/8bevrz+pTbfcbMcL5+nTpwjxm\njJrW07dubZ/WyX/a7kcUum0sfujjxjHfe29s+3fiuedCt7OOCKXfo04jRsWLfry1a2PbLtY+FnqZ\n4uWSS9Q+fvOb8HXTp6t1p54a//6dgPih+wezTXX8eHs7uV0I2XQMKOUFRO5trrHipja3cGF4xEid\nrCxj4F8dpxpZLJ4RVpNFLI1wb70VvYbudn9mUwtgbxJy06AbL04mDydi7WPh5msiEnr+7PKpm62S\nHRdIBD2NsAbIisVrwywqmQKRGtQg1fHPneyj2dkqb+aXrpOguxUbO5EpL3f/6b57d2QRiUXE/va3\n8GXxxr6Ph1gF/dxzI68vKQltDzL7r8eDfp7tzree92QPqiGCnibooWczUZjjJScnvlFqvMbphZKd\nrWqme/YYy5z8oMvL7QUqO9vozeokMoFAaICs3Fznnq8tWkQW9FhEzCnkbLKCrEVqULZjwYLI69eu\nDW+Lqgv6l2MkQU/U16UTIuhpQDDoXceXTKKmJtQXOOkNTBpOD6VucjELulPM9kAAeOaZUCEuLFQ9\nW3fujG6yuOgiY3rQIODRR+3zdd55qlHOiVhEzCmeeqymjVjRyxVrDT3ai+bgQVVh8mrYRN3MaSfo\n+stIauj1kGR9wiaKRNkJmUPDjppd5szHjCVOi96ZJxYmTbJfrptczIJuHb3HTCBgeAwxq2m3AmsW\nt2bN1HaDBxvL9M5N/fsD8+e722c0LrssfFld7c5uiFfQY3nReGk6khp6PeB3v1NuWQ8+GDndwYP+\ndwdM1NBw2dmhNXSz/7Rumnr0USWMbnn5ZWM6Wu0pO1u56DnFFddNLtOnu99nvJjFTe/w0qWL+m/R\nArjiCmP9rl3eHPO000LnvbA7u0EvX6wVBSezlhNemY4i1dBF0DOE118HvvwSeO01+/XBoPqk9TJy\nnx9xuuFzc4FLLnEeKV63a+vbz57t3AXfPMblaaepWvqkSZGHtFu7Vr2oIg0SkZUFbNmiYsgA6lpa\nhxrzCrM9We8E49Qo5/TFEmvESbPn1H//670XixNLlgB/+EPsz0YgoPzn3YZi8Mp0JI2i9QBdcMyf\n4zrBIDBhgnfjHHrFnDmJjQ6nC4Q+2lFhoXMj8NChynSwdWvkfeqCPnGifRf8ggLg7LNDl23bpsrq\nJHBFRUDXrpGPC6jrt2aNca2rq5Wg1NU2a4dd93snQdfDE5gpKFBfM7FgFqP+/WPbti507Qrcemt8\n206Y4O4LxUvTkZhc6gG6GcJO0GMJv5lIrJ+n+flA586JOdYNNxjmk7vuUrXjRo2ca8lOkROtmB8Y\nu8iFs2apLvZW9H07jY/qho0bw/OfKLc+uwBZZkHXX4zMwLBhRtq6uOj5tW9DNFNNYaG3pqNIx0tL\nQSeiUUS0jog2ENFtNus7ENFiIvqUiFYRURSP0MwnkqCny9ial18eOp+fr0K6JoLsbKPGp79IIp2H\naL7n+r6sD5NdRxenWv6uXerBbtPG2JfbBz0YdG47SMT1jaWGbp6vS4cfvwq6U76LitSXWSyN0W6w\nE239/k07kwsRZQOYAeAcAN0BjCUi66h4dwCYx8x9AVwGIIbhaTMT/WH/4YfwBz/Rbl9uOeOM0Pn8\nfGWOcEIfd9IN1lHNzT0+dXGKdB4i1Wxyc4Hjj4+eTkdPa6VDB/Vg6wGdiorcP+iRauGJuL5mYYhF\n0L06pt/RX/SJaAOwO0/WNp5k4eaSDQCwgZk3MvMhAHMBnG9JwwD0pqemAL7zLov+xCzi1lp6tB5t\nySAry35gA7tgTTpuY3ID4aOcMxs3vm4+KC939hCYNMneHDJnjgqWpNv63TwwN98cvsxsWomnJhqp\nFp5ot75kCfoHHxjTsQygko4kuwu+bopLuxo6gLYAvjXNV2rLzNwFYBwRVQJYAOA6ux0R0SQiWk5E\ny3fs2BFHdv1DTY3x4JkFPRhUnUlSSVaWElw7QY8UuzwWQbfzNNAfKr2GHggo904zhYXA008r7xI7\ne7geaVIfWMGNoF94Yei81aasC3oswu5UCy8sTLwniFXQ9dj3gPr3QryCwVAPH68646SKWH3a64pe\nQ/drLJexAJ5l5nYAzgXwVyIK2zczz2LmUmYubdWqlUeHdsfMmWp0GSs//QSMHasC47/9NnDbbSoc\n5qpVqmFu2rTI+50+XbkoAsrV6qGHgIEDld1W72TSvbtqqKqoUPt2G9u6LjRoEC52ujAWFyvhsdaA\n8/OdB1LIzq6boM+ebXy1mB+uc84JTWe2bzoFfsrLMwTdzQNjTvPll+Gf3roYxvLQl5eH177i8SSJ\nh/x8JawPP6zm160zBnbIyvJGRMrK1LNhJplxXLwm2cKq31Oxhi+oM05hGPUfgFMAvGua/y2A31rS\nrAbQ3jS/EcCxkfab7PC5ev88K//8p1p+xhmhYTVPPtmYPno0+n7nzIkeqvP442MP71mXX4sWzGee\nqX6zZxt5fuMN5pkzmQ8fVmn09F9/zXznneGheXNymNu2dQ4B26wZ86xZzBdcwDx8OPOgQcwNGoSn\ny8tjLi1lrqw08nLkCPONN6pQrX/8o7trOXIkc36+2uff/x49/datRh7Wrw9ff/Qo8y23MK9b5+74\nOv36hZavsNCb8LFO6Me57rrwsMoNG6pQv9XVzB9/7Hy/x3osu58fsOZ56lTvj3HVVWrfTz0Vvu7g\nQRXmescO74+LCOFz3Qh6jibQnQDkAfgMQA9LmrcBXKlNd4OyoVOk/aaLoC9apJaffnroDdC7tzF9\n6FD0/VofsET8Yn0huIl7PXeukX7LFrXs978P3QfAnJvL3KiR/XHuuMPYn10Md/OvLvGndfr0MfbX\nqlV0Ed2+3Ui/cWPdj6/Tv394+QoKEifq5jJHOrfr1tVdfJ3i7Wdne1KUhGPN9w03eH+MSIKeSCIJ\nelSTCzPXAJgK4F0Aa6G8WVYT0d1ENFpLdhOAa4joMwAvauLOdflySBb6Z7PVn9gcNyRSfA6dZJhR\nvouxqdmNt4X5U1S3zV5wgbFMv4qHDzubXMymHbuxTs3U1aUvGAQ+/9yY37Ejum3XnD8vG6nsGpAT\naZbQy+HU/KSf2/feM5bF25jp5Daa6lDG8ZJsk0uqcHV7M/MCZj6ZmU9k5nJt2Z3MPF+bXsPMg5m5\nmJn7MPM/EplpL9EfEuuNam4wNAu6eQCKdB5jMz/fnbeFnaDHavczC2Y0wa6rS5/dcGzRRNScP6/c\nyCKN85mofgb69XEKMNahg8qX2asn3sZMp3s7ne/5SIig1zOOHg2tlefnG7U5XdCDQfVw6ANQpEsH\nIR3zQLtu4z2bb3TdFTBa46BV8O+806gJRhJsL7pbO53zSNfCXEYvBF2/D5xIVD8DXdCnTHHu4VpW\nZjQY68Tz1VBeHh4GIhmRFhOFCHo9Qe+Cf+RIqIdHfr5xE+iCHs2ckGoee8yYtrrqOWEndtFq6Hbx\nXvSa4Lnn2gdVys/3pru1k1i67aTkhckl0n2QSNEzm8RmzTJi0eTmGuc2nheeHYFA6MDNyYq0mChE\n0OsJToLeoEG4oCerRp6VpcK2xhooy/wAuq2JRgos5ISTr3p1tRo1ZtYsIwSqztSp3oiBXWekaCLq\ntckl0n2QSNEzm8QCAeUiC6gwuvox43nhOTFypDGdrEiLiUIE3efMng089ZSKwuckQK+8YkTiswr6\nkSPGTaB/wprDsCaSSy9VnTpuuMFY1qaNEvlINkxzY9kpp7izm0YTdLuIhJHCzm7erB78//mf0OVe\nNZEHAsDw4cZ869bRRdRrQXcSx1hCB8SDbhI8dEj9mzsW6ZSX1y3gmJlM6vpfXwQ9qttion6Jdls0\nuyzdf7+9G9eAAcbynj2ZR40y5n/2M8NH+4MPlK9yotwRBw82pocPZ16xQuVv4UJj+Zo1Rr6nT2de\nsCD6ft240H34Yfi5OXTIWBYMOvufR3Kd+8Mf1HyvXsyBAPPOnXW8oCZuuME43tKl7rbR0+/ZU/fj\n27lmNmyYWB90ZuYvv2SeOJG5pkbNf/216mMQDIbnr7DQuB7x5uv99+vu/pgq5sxhfuwxI/8PPOD9\nMb7/Xt3bBw54v+9IoC5+6In6JVPQy8uN6Tlz1E1OpDq66Mu7dVOC3r8/86mnqk4axx6r1un/ifj1\n7ct8111qesCA0DJ88IGRbsOG0HVuXzDR/L6XLg1/aM37fuklwxc9lhfIgw+qZUOH1vVKhnPrrcYx\nly2Lnt7c6at9e2+Ed86cUL/8mTPrvs90w3z/+ZEjR4z8P/JIqnPjHZEEPYM+qpwxxx43e6non66A\nGqHm/feVyWDFCmDhQsOEsX174vLWrp2zGcD8mWi1a7/wgrv9R7P7232KmmOa5OS4s78SAePHGyYH\nu3EhzS6fdQn2ZN5nNBOK1SPl22+9iUkSCIQO+2Y3/qbf8bvJxZz/+mJy8fklc4dZ0CN5qfz4owof\nqzeCMic2X0BoWFkrToIezW3OTDQxjnaj5+Qo+2s04WRWDaI6VkG3unzWJdiT2QsnWr7sPFK86vxj\nFoxkh0lNBn6Nh26HCHoGYY6qiv4AACAASURBVK6JpyNON5tZxM0i5tZ90k1jmH5sp3EYc3JUbXTQ\noOjHM38NWAXdS2E1n5doD6pXbnx2JKLDUjohgu4/6oWgp2q4N7c9Lt3U0P/+d8NcsWlT9H126ODO\nhc6NoAOhQ9O5cY2zhsr1UlhjMbl46cZnJdMF3e8mFzMi6D7G2jXcLOjJjIs8cWL0kdaZ3Qn69dcb\n5gon2rc3pjdudOdCp+8vmqCb8+LGNc7t6ETxCGssJhcv3fisJCpGTLogNXT/kYG3YXgcZ7PJxSne\ndyIYNkzF95482TnNG2+ED/KgY74Jrd25rRQUqMGXddwKjO6j73ReRoxQXwbffKPmiYBx41TtXxfr\nli3Dvwasgh6vsNo1pMZSQ9cHjtbxssej+RyLoKc3Iug+xhod0VxD37Urvn3GY+vVbyJzY6Edu3er\n/5077bePhi5SZlHp1Mldg2OvXsB55wHPPGMss263aZMxHJl+jEAA6NlTTT/5ZLhA6p2PzKMTOY1A\n5IRTQ+onnxhp3Jg6zMfwssejfmzziEGZRCa9pETQfUwkQY9mAnFixYrYt/nwQ/Xv1k5cWRk6H+0m\nLCpSQldRoeanTDHWufUiycsD5s9Xoxjp2L289NGG7B5yu3zq6c3rnEYgcsKpIfW11yIfO1mYBT0T\nyaSXVNJHDkoRGXkrWgXdbHIZMya+fb79duzbzJgRPQKhGas3jlmsopkrvIqyB0R+AR0+bJg+9Afe\nrpasv0Tr0mbhlI+qKmM6lY2RupCLoKc/yR5TNFVk5K0YqYaezIfvp5+UoJ57rrv01lqEWdCjmSu8\n9CKJ9gLSa/+6qchOVP/zH/X/5z/H34ko0kDMOqkU9Ez0bDGTSS8qqaH7mEg19Llzk5uXTZuA555z\nl9bspQLEZq7w0ovEjQdIdTWwZYuatgpbMAi89JIxH28nIqeGVHOvTBH0xCE1dP+RkYJuNT2YBd7a\n8OgFkW787Gz3MdSt9v1Y7MNeuue5bTS0Rv3TKSsL9/2Px/zj1JA6bJiRJpU2dL0Gm0nCZyaTyiU1\ndB9jraEvWmRMt2zp/fFOOEH9W+OX5+XVbQxGs1hFi4OSKPe8r792DtmrPyTWmqqX5h+7L5N06dCj\nHzsZISJSQSaZXKSG7lP27AFuu815fZ8+3h9Tv1ms/u8TJrgbg9Gp9mAWdDdxUBLhnqfHcrE+EAUF\nhonIKqqJ7J1pPV46CHqmkkk19H796hYQzi9knKDfdBOwfLnz+k8/9fZ4eXmG26BOdraKolhebm8K\nyc0FmjdX023aAL/+tf2+c3KAM84AWrVKXICpaOixXC6+2Fim1/5btVLz1ppceXnoINuAt0OzpYug\nZ1IN1g6/l8+uP4UXkTbTGZ9fsnCcRifSMbu8ecGhQ+EmniNHlNAUFtrbgZ95RnVwYga++071xrSD\nSIXxdbL7J2NIPL1mPmCA+s/PN2r/TjW4QECNFhVLJ6JY8HrQ53jRj51JNVkzfi+XXYUnWRWhVJFx\ngh7tJtRrxonGLLbRPFRSGWAqGrp46nm0G37OzoYcayeiWHj/fWPabY/YRJDpNnS/C3oiI22mK/VO\n0M85J/J6q6lAH8cxVmIR21QGmIqGVdDN6Oc6mYIWDAIPP2zMp/IzWjftHTqUmfZZv5tcUlkRShU+\nv2ThRBN0Pf6IjjXKYF5eqJ9z376x5yFWsXUbYCpRJoxIWAXdLN6pEPSysvDG51R8RgeDoQKeifZZ\nv9fQ7Z7BZFWEUkW9EHRz7dYqBqecEurBsW8f8MorxvyqVbEdv7AwdrF18+meSBNGJNJN0NPlM7qs\nLDxUQ6bZZ/0u6FbieTb9RsYJuh1mDxGroC9dGt4Jxjz/ww/h+4v0KdqoUew3TDpHgtOFXM+jnQ09\nmaTLZ3S6vFgSiZ9NLnbDNEYLQZ0J+PiSKXbuBP70J6OWGK1WYf0k3r8/9mNG8nPftCn2QZDT2VtC\nf6jTxYZeXh7egSsVn9Hp8mJJJOl4P7olkWPJpjO+F/QrrgCmTTP8y6PdhN9+GzrfpEnsx8zKihyG\nN9ZBkNPxwXnqKaBHD2PezuRyzz2q52087QzxEggA//u/xrzb9oQbbwz1pa8r5eXhHcIyzT6bjvel\nW+rDF5Qdvhd0PeKfbkpxexPecIP6HzIkvBdktG7C998PXHppuOeJFT/XCK6+GvjiC2PeTtBPPx3Y\nsSO+l2JdOO88Y9pte8JDDwF/+5t3eQgEQuPPJ7OhOln42eRSH76g7PDxJVNYP/vdCvrXX6v/Ll1U\nLV+nWTPg8ssjb/vTT+rhHT/e8DxxIlNqBOlk50+XvJx9tjGdzIbqZOHnGnoqXX1Tie8FXSdWQX/v\nPfV/9Chw6qnG8ttvd1czOXJEhcUtL1f7cIrZkik1gnSKW5IuebH2Wcg0/CzoiRxLNp3xvaDr4qsL\nul7zjobe4n30aHhskHnz3O3DbFLJ9BpBuogokD55yXRB97PJBUjcWLLpTJp8vMaPXos4elQ1QH70\nUWzb/+lPoQ2cn34aPR6MGd2kot8sZWVqWYcOSswz5SZKFxEF0icvmS7ofq6h11cyRtCZlZjGE3/c\nHLBrwYLYtjWbVAKBzBFwK+litwbSJy8i6EK64fOPqlCTy6ZNdd/frl3u02aSSSUa6VIrBtInL/HG\n+fELfje51EfSpK4TP2aTS6NG9j07Y6F1a2Dbtujpioq8M6l07ar+vXBxHDrUGEHJS9JFRIH0yYvU\n0NOfiROBL79MdS6SR8YIOjPQuXPdB7B48EHgqqvC43SY+eYb1RPUK5o29a635QcfeLMfK+kiokD6\n5EUEPf2ZPTvVOUgurj6qiGgUEa0jog1EZNvxnYguJaI1RLSaiF7wNpuR8qb+mYG2beu+v0AAOO00\nY97uszNdBCWZpFOZ0yUvmS7oYnLxH1Fr6ESUDWAGgJEAKgEsI6L5zLzGlKYzgN8CGMzMu4no2ERl\n2Ip+0x09Gn/gqGOPBbZvV9PBIPCvfxnr7PaZLo1yySSdypwuecn0keQzoYZe33DzDh4AYAMzb2Tm\nQwDmAjjfkuYaADOYeTcAMPN2b7PpTF0DRI0bB8yZo6azsuzDolpJlxpiMkmnMqdLXjJd8DK9fJmI\nG0FvC8Ac0qpSW2bmZAAnE9G/iWgpEY2y2xERTSKi5US0fMeOHfHlOGyf6p85NlE3N6bqApGb666r\nfn38FE0XEQXSKy+ZTH28z/2OV5csB0BnAMMAjAUwm4iaWRMx8yxmLmXm0lb6kPF1xOy2GIug62k3\nbjQE4tAhd/uojzWXdBLRdMpLJlMf73O/40bQtwBob5pvpy0zUwlgPjMfZuZvAHwFJfAJR7/pliwB\nvvoq9u1XrQIWLVLTmTrYrxeki90aEEFPFiLo/sONoC8D0JmIOhFRHoDLAMy3pHkNqnYOImoJZYLZ\n6GE+o/L736vadqxUV4cG8bFy3HHG9G9+o/4bN479OH4nnUQ0nYSmY8fM7R0sJhf/EfWSMXMNgKkA\n3gWwFsA8Zl5NRHcT0Wgt2bsAqohoDYDFAG5h5ir7PXpLXW+6oiLg+++d15tDAdxwg6rFZ7p3gx3p\nJOjpxDffGI3qmUY6vTgFd7j6kGbmBQAWWJbdaZpmAL/RfknF7U2Xm6vSmj1Y9K77N9/sLOpmU0M6\nmR2SjQh6/UNq6P7D95fMjaBnZ6suwE8/bSwzx0eeNs15WxF0RX0ue31Fauj+w/eC/vnn0dPog1GY\nMcdH1oc002vxx5q6RZmHo6vPoiY19PqHCLr/8LWgB4NKmN0QaXzPmhr1362bEVddR2roChH0+oeY\nXPyHry9ZrNEJnToNHT6s/vXauPlGFkFXiKDXP6SG7j98LeixDsDsNL5nNEHXb2wRdKE+IYLuP3wt\n6LEMwKx7tAwcCLRvH7rupJPU/7XXqn+roN9xhzFdX0nHsl9ySapzIAjpha8Fvbw8cs2xsFDVMswe\nLUuXhtfsjz1W+ZePH6/mrYJ+991qfX2usaRbDZ3Z/WDeglBf8LWgBwJAaanz+oMHgb/+NfYRv51s\n6PWZdBN0QRDC8bWgA0CnTs7rInm2REIEPRwRdEFIf3wv6NHMILE2nAIi6HaIC5sgpD8Z/5jG0nCq\nYxYvqZkq6nP7gSD4Bd8LeqSao+7ZUpd9ipAJguAXfCfon38O/OUvRu/OSII7fnx8oU3FvCAIgh/x\nnXS9844KtPXTT2o+kqDPmhXajd8tIujOjBuX6hwIyUauuX/wXZOfLrZHj6r/SLFcjhwBJk1S0/G6\nLQoGNTVybuobcs39he8ulVXQV6yInD4e10W5ge3JzpY2hfqGXHN/4Tvp0m8uXdAPHIi+TayuiyLo\ngiD4Ed9Jly62zO7t47G6LoqgC4LgR3wnXWaTixtTSl5e7K6LZkEPBtVAwFlZ6j+eRlZBEIRk4GtB\nd2NKadw4eoOoVbTfeMNYN2kSsGmT+iLYtEnNi6gLgpCO+E7QzTb0Nm2ip6+qilzDDgbDRfv22431\n1dWh6eONDyMIgpBofO22eOhQ9PRESqQBo4YNGLX2srJw0f7xx8j7jCc+jCAIQqLxXQ3d3CiqjzTk\nBJFKZ8Zaw45HnOOJDyMIgpBofCvoR4+qjkNOFBWFi7mOWcSjiXNBQfh8PPFhBEEQEo2vBV2P52JH\nRYUSdTvMIl5eHi7aDRsa07Nmqf1YRz4SBEFIN3wn6OZG0Ug1dMBerK017EAgXLQfeMA4ViCgXg5H\nj8Y+8pEgCEIy8Z2g6zX0116LbkO3E2u7GrZVtC+9VC3Py/M694IgCInDt14ut97qLn0gEHutWj+G\nCLogCH7CtzX0SPbzuqKbckTQBUHwE74V9ESSn6/+L7ss8ccSBEHwCt+ZXJIRyrNxY2DbNqCwMPHH\nEgRB8Arf1tDthN1LE8mxx8oA0YIg+AvfCrqd2F59dXLzIgiCkE74VtDtGkXnzUtuXgRBENIJ3wq6\nHVVVycuHIAhCuuE7QZfxDQVBEOzxnaDL8HCCIAj2+E4edUFv1Ci1+RAEQUg3XAk6EY0ionVEtIGI\nbouQ7iIiYiIq9S6LoeiCPnZs+DprIC5BEIT6RFRBJ6JsADMAnAOgO4CxRNTdJl1jANcD+NjrTIYe\nR/336xe6XA+8JQiCUF9xU0MfAGADM29k5kMA5gI43ybdPQD+ACDKAG51Q6+hmyMtnniihLYVBEFw\nI+htAXxrmq/UltVCRCUA2jPzW5F2RESTiGg5ES3fsWNHzJkF7AVdPF8EQRA8aBQloiwADwG4KVpa\nZp7FzKXMXNqqVau4jieCLgiCYI8bQd8CoL1pvp22TKcxgJ4A3ieiCgCDAMxPVMOoLuiHDhnLRNAF\nQRDcCfoyAJ2JqBMR5QG4DMB8fSUz72XmlszckZk7AlgKYDQzL09EhnXxlhq6IAhCKFEFnZlrAEwF\n8C6AtQDmMfNqIrqbiEYnOoNW3JhccnwXFFgQBKHuuJI+Zl4AYIFl2Z0OaYfVPVvORDO5bN4MNGyY\nyBwIgiCkJ76ry0arobdvD0EQhHqJb7v+m2voEt9FEATBh4IujaKCIAj2+E7QxQ9dEATBHt8Kuvih\nC4IghOJbQZ8/31i2Z09q8iIIgpBO+E7Q335b/VdXG8s2bwaCwdTkRxAEIV3wnaA/9lj4MmagrCz5\neREEQUgnfCfoW7faL9+8Obn5EARBSDd8J+jHH2+/vEOH5OZDEAQh3fCdoN94Y/gyIqC8PPl5EQRB\nSCd8J+i/+IX6z8szlh1/vIxWJAiC4DtB133Ou3QxljVqlJq8CIIgpBO+E3S7nqJHj6YmL4IgCOmE\nbwXd3FNUBF0QBMHHgm6uoTOnJi+CIAjphO8E3S7a4pEjqcmLIAhCOuE7QRcbuiAIgj2+FXSxoQuC\nIITiW0Hfv99YduBAavIiCIKQTvhO0P/+9/Ble/ZItEVBEATfCbpTF3+JtigIQn3Hd4JeWWm/XKIt\nCoJQ3/GdoDdvbr9coi0KglDf8ZWgB4OhjaFmJNqiIAj1HV8JelkZUFNjv06iLQqCUN/xlaCLnVwQ\nBMGZnFRnIBY6dAA2bUp1LgTBWw4fPozKykr8+OOPqc6KkEbk5+ejXbt2yM3Ndb2NrwS9vByYMCG0\n2z8AZGenJj+C4AWVlZVo3LgxOnbsCNKDFQn1GmZGVVUVKisr0alTJ9fb+crkAthHVpRoi4Kf+fHH\nH1FYWChiLtRCRCgsLIz5q81Xgu7UKCqxXAS/I2IuWInnnvCVoEujqCAIgjO+EnTpPCQIqj9Gx44q\nUF3HjnWPY1RVVYU+ffqgT58+OO6449C2bdva+UPmsKYRmDBhAtatWxcxzYwZMxCUoEsJhThFBujS\n0lJevnx5TNsEg8CkSUB1dfg6saMLfmXt2rXo1q2bq7R2z0BBATBrljd9Me666y40atQIN998c8hy\nZgYzIyvLV3XAOlNTU4OcnNT5jtjdG0T0CTOX2qX31dUJBNSNqyPeLUJ9o6wsvEJTXZ2Y4HQbNmxA\n9+7dEQgE0KNHD2zduhWTJk1CaWkpevTogbvvvrs27ZAhQ7By5UrU1NSgWbNmuO2221BcXIxTTjkF\n27dvBwDccccdeOSRR2rT33bbbRgwYAC6dOmCjz76CABw4MABXHTRRejevTsuvvhilJaWYuXKlWF5\nmz59Ovr374+ePXvi2muvhV4x/eqrr3DGGWeguLgYJSUlqKioAADce++96NWrF4qLi1GmnSw9zwDw\n/fff46STTgIAPPXUU/jFL36B4cOH4+yzz8a+fftwxhlnoKSkBL1798abb75Zm49nnnkGvXv3RnFx\nMSZMmIC9e/fihBNOQI3W2Ld79+6Q+YSjv3mT/evXrx/HS24uM8DcsaP6B+LelSCknDVr1rhOS2Tc\n8+YfkTd5mT59Ot9///3MzLx+/XomIl62bFnt+qqqKmZmPnz4MA8ZMoRXr17NzMyDBw/mTz/9lA8f\nPswAeMGCBczMfOONN/J9993HzMxlZWX88MMP16a/9dZbmZn59ddf57PPPpuZme+77z6eMmUKMzOv\nXLmSs7Ky+NNPPw3Lp56Po0eP8mWXXVZ7vJKSEp4/fz4zMx88eJAPHDjA8+fP5yFDhnB1dXXItnqe\nmZm3bt3KJ554IjMzz549mzt06MC7du1iZuZDhw7x3r17mZl527ZtfNJJJ9Xmr0uXLrX70//HjRvH\nb7zxBjMzz5gxo7ac8WB3bwBYzg666qsauo7+1ZeXl9p8CEKycWpHSlT70oknnojSUuPr/sUXX0RJ\nSQlKSkqwdu1arFmzJmybhg0b4pxzzgEA9OvXr7aWbOXCCy8MS/Phhx/isssuAwAUFxejR48ettsu\nXLgQAwYMQHFxMT744AOsXr0au3fvxs6dO3HeeecBUB1zCgoK8N577+Gqq65Cw4YNAQAtWrSIWu6z\nzjoLzbVIgMyM2267Db1798ZZZ52Fb7/9Fjt37sSiRYswZsyY2v3p/xMnTsQzzzwDQNXgJ0yYEPV4\nXuFK0IloFBGtI6INRHSbzfrfENEaIlpFRAuJqMj7rBrogt6gQSKPIgjpR3m5spmbKShIXHC6Y445\npnZ6/fr1ePTRR7Fo0SKsWrUKo0aNsvWTzjPVtLKzsx3NDQ20BzhSGjuqq6sxdepUvPrqq1i1ahWu\nuuqquHrZ5uTk4Kjm82zd3lzu559/Hnv37sWKFSuwcuVKtGzZMuLxTj/9dHz11VdYvHgxcnNz0bVr\n15jzFi9RBZ2IsgHMAHAOgO4AxhJRd0uyTwGUMnNvAC8D+KPXGTWjX/t61j4jCLXtSEVFAJH696pB\nNBr79u1D48aN0aRJE2zduhXvvvuu58cYPHgw5s2bBwD4/PPPbb8ADh48iKysLLRs2RL79+/HK6+8\nAgBo3rw5WrVqhTfeeAOAEunq6mqMHDkSTz/9NA4ePAgA2LVrFwCgY8eO+OSTTwAAL7/8smOe9u7d\ni2OPPRY5OTn45z//iS1btgAAzjjjDLz00ku1+9P/AWDcuHEIBAJJrZ0D7mroAwBsYOaNzHwIwFwA\n55sTMPNiZtabapYCaOdtNkM57TT1v29fIo8iCOlJIABUVKgOdRUVyYs0WlJSgu7du6Nr16644oor\nMHjwYM+Pcd1112HLli3o3r07fve736F79+5o2rRpSJrCwkKMHz8e3bt3xznnnIOBAwfWrgsGg3jw\nwQfRu3dvDBkyBDt27MDPf/5zjBo1CqWlpejTpw8efvhhAMAtt9yCRx99FCUlJdi9e7djnn75y1/i\no48+Qq9evTB37lx07twZgDIJ3XrrrRg6dCj69OmDW265pXabQCCAvXv3YsyYMV6enqhEdVskoosB\njGLmidr8LwEMZOapDukfB/A9M/+fzbpJACYBQIcOHfptijPS1rx5wJgxQOvWwLZtapm4LQp+JRa3\nxUynpqYGNTU1yM/Px/r163HWWWdh/fr1KXUdjIe5c+fi3XffrbWlx0usboueniUiGgegFMDpduuZ\neRaAWYDyQ4/3OM2aqX87f3RBEPzLDz/8gBEjRqCmpgbMjJkzZ/pOzCdPnoz33nsP77zzTtKP7eZM\nbQHQ3jTfTlsWAhGdCaAMwOnM/JM32bNHF3SJNioImUWzZs1q7dp+5YknnkjZsd3Y0JcB6ExEnYgo\nD8BlAOabExBRXwAzAYxm5u3eZzMUXdCtYXQFQRDqM1EFnZlrAEwF8C6AtQDmMfNqIrqbiEZrye4H\n0AjA34hoJRHNd9idJ+iCLgiCIBi4Mk4x8wIACyzL7jRNn+lxviIigi4IghCOLz25pYeoIAhCOL4U\ndAB49lngs89SnQtB8D/Dhw8P6yT0yCOPYPLkyRG3a9SoEQDgu+++w8UXX2ybZtiwYYgWVfWRRx5B\ntcll7dxzz8WePXvcZF2w4FtBHz8e6N071bkQBP8zduxYzJ07N2TZ3LlzMXbsWFfbH3/88RF7WkbD\nKugLFixAMx/ZVZm5NoRAqvGtoAtCJnLDDcCwYd7+brgh8jEvvvhivPXWW7WDWVRUVOC7777Daaed\nVusXXlJSgl69euH1118P276iogI9e/YEoLrlX3bZZejWrRsuuOCC2u72gPLP1kPvTp8+HQDw2GOP\n4bvvvsPw4cMxfPhwAKpL/s6dOwEADz30EHr27ImePXvWht6tqKhAt27dcM0116BHjx4466yzQo6j\n88Ybb2DgwIHo27cvzjzzTGzTeiH+8MMPmDBhAnr16oXevXvXhg545513UFJSguLiYowYMQKAig//\nwAMP1O6zZ8+eqKioQEVFBbp06YIrrrgCPXv2xLfffmtbPgBYtmwZTj31VBQXF2PAgAHYv38/hg4d\nGhIWeMiQIfjMA5ODrzz2g0EV93nzZhVdLlEBiQShPtGiRQsMGDAAb7/9Ns4//3zMnTsXl156KYgI\n+fn5ePXVV9GkSRPs3LkTgwYNwujRox3Hu3ziiSdQUFCAtWvXYtWqVSgpKaldV15ejhYtWuDIkSMY\nMWIEVq1ahWnTpuGhhx7C4sWL0bJly5B9ffLJJ3jmmWfw8ccfg5kxcOBAnH766WjevDnWr1+PF198\nEbNnz8all16KV155BePGjQvZfsiQIVi6dCmICE899RT++Mc/4sEHH8Q999yDpk2b4vPPPwegYpbv\n2LED11xzDZYsWYJOnTqFxGVxYv369XjuuecwaNAgx/J17doVY8aMwUsvvYT+/ftj3759aNiwIa6+\n+mo8++yzeOSRR/DVV1/hxx9/RHFxcUzXzQ7fCLp1pJZNm9S8IGQSWiU06ehmF13Q//KXvwBQ5oTb\nb78dS5YsQVZWFrZs2YJt27bhuOOOs93PkiVLMG3aNABA79690dtkF503bx5mzZqFmpoabN26FWvW\nrAlZb+XDDz/EBRdcUBv58MILL8S//vUvjB49Gp06dUKfPn0AOIforaysxJgxY7B161YcOnQInTp1\nAgC89957ISam5s2b44033sDQoUNr07gJsVtUVFQr5k7lIyK0adMG/fv3BwA0adIEAHDJJZfgnnvu\nwf3334+nn34aV155ZdTjucE3JhenkVoEQag7559/PhYuXIgVK1aguroa/fr1A6CCXe3YsQOffPIJ\nVq5cidatW8cVqvabb77BAw88gIULF2LVqlX42c9+Ftd+dBqYYmc7hd+97rrrMHXqVHz++eeYOXNm\nnUPsAqFhds0hdmMtX0FBAUaOHInXX38d8+bNQ8CjCGu+EfTNm1OdA0HIXBo1aoThw4fjqquuCmkM\n1UPH5ubmYvHixYgWUG/o0KF44YUXAABffPEFVq1aBUCF3j3mmGPQtGlTbNu2DW+//XbtNo0bN8b+\n/fvD9nXaaafhtddeQ3V1NQ4cOIBXX30Vp+mhVl2wd+9etG3bFgDw3HPP1S4fOXIkZsyYUTu/e/du\nDBo0CEuWLME333wDIDTE7ooVKwAAK1asqF1vxal8Xbp0wdatW7Fs2TIAwP79+2tfPhMnTsS0adPQ\nv3//2sE06opvBD1RI7IIgqAYO3YsPvvssxBBDwQCWL58OXr16oXnn38+6mANkydPxg8//IBu3brh\nzjvvrK3pFxcXo2/fvujatSsuv/zykNC7kyZNwqhRo2obRXVKSkpw5ZVXYsCAARg4cCAmTpyIvn37\nui7PXXfdhUsuuQT9+vULsc/fcccd2L17N3r27Ini4mIsXrwYrVq1wqxZs3DhhReiuLi4NuztRRdd\nhF27dqFHjx54/PHHcfLJJ9sey6l8eXl5eOmll3DdddehuLgYI0eOrK259+vXD02aNPE0ZnrU8LmJ\norS0lKP5p5pxGu187Fjg6quBU05JQCYFIQlI+Nz6yXfffYdhw4bhyy+/RJbDaD2xhs/1TQ3daaSW\np54SMRcEwV88//zzGDhwIMrLyx3FPB58U0MXhExFauiCExlbQxeETCZVFSshfYnnnhBBF4QUk5+f\nj6qqKhF1oRZmRlVVFfLz82PazjcdiwQhU2nXrh0qKyuxY8eOVGdFSCPy8/PRrl27mLYRQReEFJOb\nm1vbQ1EQ6oKYXARBhN+rgAAABI9JREFUEDIEEXRBEIQMQQRdEAQhQ0iZHzoR7QAQOTCEMy0B7PQw\nO6lEypKeSFnSj0wpB1C3shQxcyu7FSkT9LpARMudHOv9hpQlPZGypB+ZUg4gcWURk4sgCEKGIIIu\nCIKQIfhV0GelOgMeImVJT6Qs6UemlANIUFl8aUMXBEEQwvFrDV0QBEGwIIIuCIKQIfhO0IloFBGt\nI6INRHRbqvMTDSJ6moi2E9EXpmUtiOifRLRe+2+uLSciekwr2yoiKkldzkMhovZEtJiI1hDRaiK6\nXlvux7LkE9F/iegzrSy/05Z3IqKPtTy/RER52vIG2vwGbX3HVObfDiLKJqJPiehNbd6XZSGiCiL6\nnIhWEtFybZkf77FmRPQyEX1JRGuJ6JRklMNXgk5E2QBmADgHQHcAY4moe2pzFZVnAYyyLLsNwEJm\n7gxgoTYPqHJ11n6TADyRpDy6oQbATczcHcAgAL/Wzr0fy/ITgDOYuRhAHwCjiGgQgD8AeJiZTwKw\nG8DVWvqrAezWlj+spUs3rgew1jTv57IMZ+Y+Jj9tP95jjwJ4h5m7AiiGujaJLwcz++YH4BQA75rm\nfwvgt6nOl4t8dwTwhWl+HYA22nQbAOu06ZkAxtqlS7cfgNcBjPR7WQAUAFgBYCBUz70c670G4F0A\np2jTOVo6SnXeTWVopwnEGQDeBEA+LksFgJaWZb66xwA0BfCN9bwmoxy+qqEDaAvgW9N8pbbMb7Rm\n5q3a9PcAWmvTviif9pneF8DH8GlZNBPFSgDbAfwTwNcA9jBzjZbEnN/asmjr9wIoTG6OI/IIgFsB\nHNXmC+HfsjCAfxDRJ0Q0SVvmt3usE4AdAJ7RzGBPEdExSEI5/CboGQerV7JvfEeJqBGAVwDcwMz7\nzOv8VBZmPsLMfaBqtwMAdE1xluKCiH4OYDszf5LqvHjEEGYugTJD/JqIhppX+uQeywFQAuAJZu4L\n4AAM8wqAxJXDb4K+BUB703w7bZnf2EZEbQBA+9+uLU/r8hFRLpSYB5n579piX5ZFh5n3AFgMZZZo\nRkT6oC/m/NaWRVvfFEBVkrPqxGAAo4moAsBcKLPLo/BnWcDMW7T/7QBehXrZ+u0eqwRQycwfa/Mv\nQwl8wsvhN0FfBqCz1oKfB+AyAPNTnKd4mA9gvDY9HsoerS+/Qmv1HgRgr+kTLaUQEQH4C4C1zPyQ\naZUfy9KKiJpp0w2h2gLWQgn7xVoya1n0Ml4MYJFWw0o5zPxbZm7HzB2hnodFzByAD8tCRMcQUWN9\nGsBZAL6Az+4xZv4ewLdE1EVbNALAGiSjHKluQIijweFcAF9B2TzLUp0fF/l9EcBWAIeh3txXQ9ks\nFwJYD+A9AC20tATlxfM1gM8BlKY6/6ZyDIH6RFwFYKX2O9enZekN4FOtLF8AuFNbfgKA/wLYAOBv\nABpoy/O1+Q3a+hNSXQaHcg0D8KZfy6Ll+TPtt1p/vn16j/UBsFy7x14D0DwZ5ZCu/4IgCBmC30wu\ngiAIggMi6IIgCBmCCLogCEKGIIIuCIKQIYigC4IgZAgi6IIgCBmCCLogCEKG8P87mMzoW7OkZQAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEICAYAAAB25L6yAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1IZhGAHHhNowDCPg/D8d/vgOo6i+IAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = model.history.history['acc']\n",
    "val_accuracy = model.history.history['val_acc']\n",
    "loss = model.history.history['loss']\n",
    "val_loss = model.history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4PmptQhovHv"
   },
   "source": [
    "**Analysis on viz.** - We have got better graphs where Both training and validation loss go hand in hand. But here because of more number of epochs, the graphs look like they are overfitting by some amount. But we took the decision to go ahead with the model, as most of the test dataset will be normalized between [0,1), and will work better with this model, than the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQ4heMhF9AIq"
   },
   "source": [
    "# The End\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MC Assignment 2 - Neural Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
